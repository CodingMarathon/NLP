{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq\n",
    "===\n",
    "\n",
    "![Snip20191021_16.png](https://i.loli.net/2019/10/21/DUIFeqGbgnMYS3L.png)\n",
    "![Snip20191021_15.png](https://i.loli.net/2019/10/21/QeEPNoL6HF4Wn8q.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果没有安装 keras 和 tensorflow 库\n",
    "#### 请使用 pip install keras tensorflow 安装\n",
    "#### 如果使用conda虚拟环境\n",
    "#### 请使用conda install -c conda-forge keras\n",
    "#### conda install -c conda-forge tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  \n",
    "epochs = 9\n",
    "latent_dim = 256  \n",
    "embedding_size = 128\n",
    "file_name = '../input/poetry.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面这段代码用于处理原始数据\n",
    "#### seq2seq的训练数据是由输入和目标组成的一对，即input和target\n",
    "#### 我们这里展示的任务是对诗，那么input是上句诗，target就是下句诗\n",
    "#### 我们首先建立所有输入句子的词典input_vocab和target_vocab\n",
    "#### 其次，解码的时候需要起始字符<BOS\\>和结束字符<EOS\\>，这里分别用制表符'\\t'和回车符'\\n'来表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4767\n",
      "7\n",
      "4816\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_vocab = set()\n",
    "target_vocab = set()\n",
    "with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    # 将诗句用逗号分开\n",
    "    line_sp = line.strip().split('，')\n",
    "    # 如果诗中不含逗号，这句诗我们就不用了\n",
    "    if len(line_sp) < 2:\n",
    "        continue\n",
    "    # 上句为input_text，下句为target_text\n",
    "    input_text, target_text = line_sp[0], line_sp[1]\n",
    "    # 在下句前后开始字符和结束字符\n",
    "    target_text = '\\t' + target_text[:-1] + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    # 统计输入侧的词汇表和输出侧的词汇表\n",
    "    for ch in input_text:\n",
    "        if ch not in input_vocab:\n",
    "            input_vocab.add(ch)\n",
    "    for ch in target_text:\n",
    "        if ch not in target_vocab:\n",
    "            target_vocab.add(ch)\n",
    "\n",
    "# 建立字典和反向字典\n",
    "input_vocab = dict([(char, i) for i, char in enumerate(input_vocab)])\n",
    "target_vocab = dict([(char, i) for i, char in enumerate(target_vocab)])\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_vocab.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_vocab.items())\n",
    "\n",
    "# 输入侧词汇表大小\n",
    "encoder_vocab_size = len(input_vocab)\n",
    "# 最长输入句子长度\n",
    "encoder_len = max([len(sentence) for sentence in input_texts])\n",
    "# 输出侧词汇表大小\n",
    "decoder_vocab_size = len(target_vocab)\n",
    "# 最长输出句子长度\n",
    "decoder_len = max([len(sentence) for sentence in target_texts])\n",
    "\n",
    "print(encoder_vocab_size)\n",
    "print(encoder_len)\n",
    "print(decoder_vocab_size)\n",
    "print(decoder_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面这段代码用于构建训练数据\n",
    "![](https://docs.chainer.org/en/stable/_images/seq2seq.png)\n",
    "#### 训练数据由三部分构成，编码器输入，解码器输入，解码器目标\n",
    "#### 即encoder_input, decoder_input, decoder_target、\n",
    "#### 在构建的同时还把字转化成了字典里的编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72514, 7)\n",
      "(72514, 9)\n",
      "(72514, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.zeros((len(input_texts), encoder_len), dtype='int')\n",
    "decoder_input_data = np.zeros((len(input_texts), decoder_len), dtype='int')\n",
    "decoder_target_data = np.zeros((len(input_texts), decoder_len, 1), dtype='int')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t] = input_vocab[char]\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t] = target_vocab[char]\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, 0] = target_vocab[char]\n",
    "            \n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面这段代码用于搭建模型\n",
    "#### ![](https://docs.chainer.org/en/stable/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    610176      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 128)    616448      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 394240      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  394240      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 4816)   1237712     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,252,816\n",
      "Trainable params: 3,252,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 编码器输入层\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "# 编码器词嵌入层\n",
    "encoder_embedding = Embedding(input_dim=encoder_vocab_size, output_dim=embedding_size, trainable=True)(encoder_inputs)\n",
    "# 编码器长短期记忆网络层\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "# 编码器长短期记忆网络输出是一个三元组(encoder_outputs, state_h, state_c)\n",
    "# encoder_outputs是长短期记忆网络每个时刻的输出构成的序列\n",
    "# state_h和state_c是长短期记忆网络最后一个时刻的隐状态和细胞状态\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
    "# 我们会把state_h和state_c作为解码器长短期记忆网络的初始状态，之前我们所说的状态向量的传递就是这样实现的\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 解码器网络建构\n",
    "\n",
    "# 解码器输入层\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "# 解码器词嵌入层\n",
    "decoder_embedding = Embedding(input_dim=decoder_vocab_size, output_dim=embedding_size, trainable=True)(decoder_inputs)\n",
    "# 解码器长短期记忆网络层\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "# 解码器长短期记忆网络的输出也是三元组，但我们只关心三元组的第一维，同时我们在这里设置了解码器长短期记忆网络的初始状态\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# 解码器输出经过一个隐层softmax变换转换为对各类别的概率估计\n",
    "decoder_dense = Dense(decoder_vocab_size, activation='softmax')\n",
    "# 解码器输出层\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# 总模型，接受编码器和解码器输入，得到解码器长短期记忆网络输出\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58011 samples, validate on 14503 samples\n",
      "Epoch 1/9\n",
      "58011/58011 [==============================] - 15s 255us/step - loss: 3.9766 - val_loss: 3.7704\n",
      "Epoch 2/9\n",
      "58011/58011 [==============================] - 13s 227us/step - loss: 3.6544 - val_loss: 3.5933\n",
      "Epoch 3/9\n",
      "58011/58011 [==============================] - 13s 227us/step - loss: 3.4701 - val_loss: 3.4549\n",
      "Epoch 4/9\n",
      "58011/58011 [==============================] - 13s 228us/step - loss: 3.3023 - val_loss: 3.3684\n",
      "Epoch 5/9\n",
      "58011/58011 [==============================] - 13s 227us/step - loss: 3.1695 - val_loss: 3.3098\n",
      "Epoch 6/9\n",
      "58011/58011 [==============================] - 13s 229us/step - loss: 3.0556 - val_loss: 3.2697\n",
      "Epoch 7/9\n",
      "58011/58011 [==============================] - 13s 228us/step - loss: 2.9514 - val_loss: 3.2489\n",
      "Epoch 8/9\n",
      "58011/58011 [==============================] - 14s 235us/step - loss: 2.8551 - val_loss: 3.2304\n",
      "Epoch 9/9\n",
      "58011/58011 [==============================] - 13s 230us/step - loss: 2.7646 - val_loss: 3.2255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7faba9e4ada0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这里可能同学们会很困惑，为什么下面这段代码又在构建模型，原因是seq2seq在训练和生成的时候并不完全相同\n",
    "#### 训练的时候，解码器是有预先输入的，我们会把正确的下句作为输入指导解码器进行学习，具体来说，不管上一个时刻解码器的输出是什么，我们都用预先给定的输入作为本时刻的输入\n",
    "#### 这种训练方式称为Teacher forcing\n",
    "#### 但是在生成的时候，解码器是没有预先输入的，我们会把上一个时刻解码器的输出作为本时刻的输入，如此迭代的生成句子\n",
    "#### 训练的时候我们的model是一整个seq2seq的模型，这个黑盒在给定encoder_input和decoder_input的情况下可以产生对应的输出\n",
    "#### 但是生成时我们没有decoder_input，我们就把黑盒拆成两个黑盒，一个是编码器，一个是解码器，方便我们的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个黑盒，编码器，给定encoder_inputs，得到encoder的状态\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "# 第二个黑盒，解码器\n",
    "# 解码器接受三个输入，两个是初始状态，一个是之前已经生成的文本\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# 解码器产生三个输出，两个当前状态，一个是每个时刻的输出，其中最后一个时刻的输出可以用来计算下一个字\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这段代码就实现了迭代的解码\n",
    "#### 假设我们已经生成了前n个字，我们把前n个字作为输入，得到第n+1个字，再把这n+1个字作为输入，得到第n+2个字，以此类推"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 先把上句输入编码器得到编码的中间向量，这个中间向量将是解码器的初始状态向量\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # 初始的解码器输入是开始符'\\t'\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_vocab['\\t']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    # 迭代解码\n",
    "    while not stop_condition:\n",
    "        # 把当前的解码器输入和当前的解码器状态向量送进解码器\n",
    "        # 得到对下一个时刻的预测和新的解码器状态向量\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        # 采样出概率最大的那个字作为下一个时刻的输入\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        # 如果采样到了结束符或者生成的句子长度超过了decoder_len，就停止生成\n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > decoder_len):\n",
    "            stop_condition = True\n",
    "        # 否则我们更新下一个时刻的解码器输入和解码器状态向量\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 辞枝枝暂起\n",
      "Decoded sentence: 远雁入寒林\n",
      "\n",
      "-\n",
      "Input sentence: 向日终难托\n",
      "Decoded sentence: 何人问此生\n",
      "\n",
      "-\n",
      "Input sentence: 只待纤纤手\n",
      "Decoded sentence: 无人问旧游\n",
      "\n",
      "-\n",
      "Input sentence: 骏骨饮长泾\n",
      "Decoded sentence: 金壶自可论\n",
      "\n",
      "-\n",
      "Input sentence: 细纹连喷聚\n",
      "Decoded sentence: 红粉拂尘埃\n",
      "\n",
      "-\n",
      "Input sentence: 水光鞍上侧\n",
      "Decoded sentence: 山月照寒沙\n",
      "\n",
      "-\n",
      "Input sentence: 翻似天池里\n",
      "Decoded sentence: 空濛碧树新\n",
      "\n",
      "-\n",
      "Input sentence: 阶兰凝曙霜\n",
      "Decoded sentence: 水色含清辉\n",
      "\n",
      "-\n",
      "Input sentence: 露浓晞晚笑\n",
      "Decoded sentence: 风暖叶初来\n",
      "\n",
      "-\n",
      "Input sentence: 细叶凋轻翠\n",
      "Decoded sentence: 清风入洞庭\n",
      "\n",
      "-\n",
      "Input sentence: 还持今岁色\n",
      "Decoded sentence: 不觉夜钟催\n",
      "\n",
      "-\n",
      "Input sentence: 秋露凝高掌\n",
      "Decoded sentence: 寒山月照明\n",
      "\n",
      "-\n",
      "Input sentence: 参差丽双阙\n",
      "Decoded sentence: 落日照金茎\n",
      "\n",
      "-\n",
      "Input sentence: 仙驭随轮转\n",
      "Decoded sentence: 金炉压柳条\n",
      "\n",
      "-\n",
      "Input sentence: 临波光定彩\n",
      "Decoded sentence: 落日照金台\n",
      "\n",
      "-\n",
      "Input sentence: 还当葵霍志\n",
      "Decoded sentence: 不觉别离情\n",
      "\n",
      "-\n",
      "Input sentence: 半月无双影\n",
      "Decoded sentence: 寒山独自伤\n",
      "\n",
      "-\n",
      "Input sentence: 摧藏千里态\n",
      "Decoded sentence: 落日照青山\n",
      "\n",
      "-\n",
      "Input sentence: 促节萦红袖\n",
      "Decoded sentence: 清风入翠微\n",
      "\n",
      "-\n",
      "Input sentence: 驶弹风响急\n",
      "Decoded sentence: 落日照金台\n",
      "\n",
      "-\n",
      "Input sentence: 空余关陇恨\n",
      "Decoded sentence: 不觉别离情\n",
      "\n",
      "-\n",
      "Input sentence: 驱马出辽阳\n",
      "Decoded sentence: 青山空有心\n",
      "\n",
      "-\n",
      "Input sentence: 对敌六奇举\n",
      "Decoded sentence: 还应问所思\n",
      "\n",
      "-\n",
      "Input sentence: 斩鲸澄碧海\n",
      "Decoded sentence: 高枕入青山\n",
      "\n",
      "-\n",
      "Input sentence: 昔去兰萦翠\n",
      "Decoded sentence: 还应上苑回\n",
      "\n",
      "-\n",
      "Input sentence: 云芝浮碎叶\n",
      "Decoded sentence: 山月照寒沙\n",
      "\n",
      "-\n",
      "Input sentence: 回首长安道\n",
      "Decoded sentence: 空山独自伤\n",
      "\n",
      "-\n",
      "Input sentence: 四时运灰琯\n",
      "Decoded sentence: 不觉寒松里\n",
      "\n",
      "-\n",
      "Input sentence: 送寒余雪尽\n",
      "Decoded sentence: 春色满山川\n",
      "\n",
      "-\n",
      "Input sentence: 焰听风来动\n",
      "Decoded sentence: 风吹不可留\n",
      "\n",
      "-\n",
      "Input sentence: 镇下千行泪\n",
      "Decoded sentence: 人间不是家\n",
      "\n",
      "-\n",
      "Input sentence: 九龙蟠焰动\n",
      "Decoded sentence: 万里一开颜\n",
      "\n",
      "-\n",
      "Input sentence: 即此流高殿\n",
      "Decoded sentence: 无人问旧游\n",
      "\n",
      "-\n",
      "Input sentence: 上弦明月半\n",
      "Decoded sentence: 万里一帆孤\n",
      "\n",
      "-\n",
      "Input sentence: 落雁带书惊\n",
      "Decoded sentence: 秋风入洞房\n",
      "\n",
      "-\n",
      "Input sentence: 初秋玉露清\n",
      "Decoded sentence: 不觉寒松色\n",
      "\n",
      "-\n",
      "Input sentence: 隔云时乱影\n",
      "Decoded sentence: 高树落花迟\n",
      "\n",
      "-\n",
      "Input sentence: 岸曲丝阴聚\n",
      "Decoded sentence: 风吹叶叶黄\n",
      "\n",
      "-\n",
      "Input sentence: 还将眉里翠\n",
      "Decoded sentence: 不觉梦中人\n",
      "\n",
      "-\n",
      "Input sentence: 贞条障曲砌\n",
      "Decoded sentence: 红粉拂香茵\n",
      "\n",
      "-\n",
      "Input sentence: 拂牖分龙影\n",
      "Decoded sentence: 开花拂绮罗\n",
      "\n",
      "-\n",
      "Input sentence: 散影玉阶柳\n",
      "Decoded sentence: 含香满碧林\n",
      "\n",
      "-\n",
      "Input sentence: 微形藏叶里\n",
      "Decoded sentence: 落日照青山\n",
      "\n",
      "-\n",
      "Input sentence: 盘根直盈渚\n",
      "Decoded sentence: 清净不能变\n",
      "\n",
      "-\n",
      "Input sentence: 舒华光四海\n",
      "Decoded sentence: 落日照金河\n",
      "\n",
      "-\n",
      "Input sentence: 近谷交萦蕊\n",
      "Decoded sentence: 开门对月明\n",
      "\n",
      "-\n",
      "Input sentence: 径细无全磴\n",
      "Decoded sentence: 山深不见人\n",
      "\n",
      "-\n",
      "Input sentence: 疾风知劲草\n",
      "Decoded sentence: 不觉别离情\n",
      "\n",
      "-\n",
      "Input sentence: 勇夫安识义\n",
      "Decoded sentence: 不觉老夫名\n",
      "\n",
      "-\n",
      "Input sentence: 太液仙舟迥\n",
      "Decoded sentence: 登临御史骢\n",
      "\n",
      "-\n",
      "Input sentence: 未晓征车度\n",
      "Decoded sentence: 孤灯对月明\n",
      "\n",
      "-\n",
      "Input sentence: 烟生遥岩隐\n",
      "Decoded sentence: 水色上林峦\n",
      "\n",
      "-\n",
      "Input sentence: 连山惊鸟乱\n",
      "Decoded sentence: 远水落花前\n",
      "\n",
      "-\n",
      "Input sentence: 醽醁胜兰生\n",
      "Decoded sentence: 金闺怨秋色\n",
      "\n",
      "-\n",
      "Input sentence: 千日醉不醒\n",
      "Decoded sentence: 一杯无不知\n",
      "\n",
      "-\n",
      "Input sentence: 雪耻酬百王\n",
      "Decoded sentence: 风吹不可绊\n",
      "\n",
      "-\n",
      "Input sentence: 昔乘匹马去\n",
      "Decoded sentence: 何处是归期\n",
      "\n",
      "-\n",
      "Input sentence: 近日毛虽暖\n",
      "Decoded sentence: 孤云远不归\n",
      "\n",
      "-\n",
      "Input sentence: 温渚停仙跸\n",
      "Decoded sentence: 青山白首新\n",
      "\n",
      "-\n",
      "Input sentence: 路曲回轮影\n",
      "Decoded sentence: 山连白日斜\n",
      "\n",
      "-\n",
      "Input sentence: 暖溜惊湍驶\n",
      "Decoded sentence: 寒山月影斜\n",
      "\n",
      "-\n",
      "Input sentence: 林黄疏叶下\n",
      "Decoded sentence: 山月照寒山\n",
      "\n",
      "-\n",
      "Input sentence: 眺听良无已\n",
      "Decoded sentence: 人间不得知\n",
      "\n",
      "-\n",
      "Input sentence: 停轩观福殿\n",
      "Decoded sentence: 清跸上龙城\n",
      "\n",
      "-\n",
      "Input sentence: 法轮含日转\n",
      "Decoded sentence: 金鼎出金台\n",
      "\n",
      "-\n",
      "Input sentence: 翠烟香绮阁\n",
      "Decoded sentence: 金缕拂花开\n",
      "\n",
      "-\n",
      "Input sentence: 幡虹遥合彩\n",
      "Decoded sentence: 玉树荫春风\n",
      "\n",
      "-\n",
      "Input sentence: 萧然登十地\n",
      "Decoded sentence: 不觉有风尘\n",
      "\n",
      "-\n",
      "Input sentence: 日宫开万仞\n",
      "Decoded sentence: 山月照明光\n",
      "\n",
      "-\n",
      "Input sentence: 花盖飞团影\n",
      "Decoded sentence: 山禽落日斜\n",
      "\n",
      "-\n",
      "Input sentence: 绮霞遥笼帐\n",
      "Decoded sentence: 红蕉叶色黄\n",
      "\n",
      "-\n",
      "Input sentence: 寥廓烟云表\n",
      "Decoded sentence: 清风入洞庭\n",
      "\n",
      "-\n",
      "Input sentence: 今宵冬律尽\n",
      "Decoded sentence: 不觉夜钟催\n",
      "\n",
      "-\n",
      "Input sentence: 花余凝地雪\n",
      "Decoded sentence: 山色上楼台\n",
      "\n",
      "-\n",
      "Input sentence: 绶吐芽犹嫩\n",
      "Decoded sentence: 山中见有人\n",
      "\n",
      "-\n",
      "Input sentence: 薄红梅色冷\n",
      "Decoded sentence: 高枕绿云生\n",
      "\n",
      "-\n",
      "Input sentence: 送迎交两节\n",
      "Decoded sentence: 遥见白云端\n",
      "\n",
      "-\n",
      "Input sentence: 九日正乘秋\n",
      "Decoded sentence: 相逢不见家\n",
      "\n",
      "-\n",
      "Input sentence: 泛桂迎尊满\n",
      "Decoded sentence: 含香满紫微\n",
      "\n",
      "-\n",
      "Input sentence: 长房萸早熟\n",
      "Decoded sentence: 高树带寒烟\n",
      "\n",
      "-\n",
      "Input sentence: 何藉龙沙上\n",
      "Decoded sentence: 不见山下人\n",
      "\n",
      "-\n",
      "Input sentence: 四郊秦汉国\n",
      "Decoded sentence: 万里一帆飞\n",
      "\n",
      "-\n",
      "Input sentence: 阊阖雄里閈\n",
      "Decoded sentence: 玉箸出尘埃\n",
      "\n",
      "-\n",
      "Input sentence: 贯渭称天邑\n",
      "Decoded sentence: 登楼望九州\n",
      "\n",
      "-\n",
      "Input sentence: 金门披玉馆\n",
      "Decoded sentence: 玉箸出金台\n",
      "\n",
      "-\n",
      "Input sentence: 眷言君失德\n",
      "Decoded sentence: 不觉有风尘\n",
      "\n",
      "-\n",
      "Input sentence: 政烦方改篆\n",
      "Decoded sentence: 人事不知名\n",
      "\n",
      "-\n",
      "Input sentence: 阿房久已灭\n",
      "Decoded sentence: 玉箸自相侵\n",
      "\n",
      "-\n",
      "Input sentence: 欲厌东南气\n",
      "Decoded sentence: 无人问此生\n",
      "\n",
      "-\n",
      "Input sentence: 有隋政昏虐\n",
      "Decoded sentence: 不知身不知\n",
      "\n",
      "-\n",
      "Input sentence: 先圣按剑起\n",
      "Decoded sentence: 清光满太明\n",
      "\n",
      "-\n",
      "Input sentence: 饮马河洛竭\n",
      "Decoded sentence: 一杯空自清\n",
      "\n",
      "-\n",
      "Input sentence: 克敌睿图就\n",
      "Decoded sentence: 清光动清漪\n",
      "\n",
      "-\n",
      "Input sentence: 顾惭嗣宝历\n",
      "Decoded sentence: 不知身未知\n",
      "\n",
      "-\n",
      "Input sentence: 幸过翦鲸地\n",
      "Decoded sentence: 不知身在斯\n",
      "\n",
      "-\n",
      "Input sentence: 汉家重东郡\n",
      "Decoded sentence: 白首一沾衣\n",
      "\n",
      "-\n",
      "Input sentence: 黎庶既蕃殖\n",
      "Decoded sentence: 不知心自知\n",
      "\n",
      "-\n",
      "Input sentence: 远别初首路\n",
      "Decoded sentence: 相逢无处时\n",
      "\n",
      "-\n",
      "Input sentence: 课成应第一\n",
      "Decoded sentence: 应是故人期\n",
      "\n",
      "-\n",
      "Input sentence: 北风吹同云\n",
      "Decoded sentence: 白首无人心\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(200, 300):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
