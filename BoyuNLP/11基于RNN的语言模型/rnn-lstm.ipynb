{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN & LSTM\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snip20191021_8.png](https://i.loli.net/2019/10/21/gTevq8kfHCOmljd.png)\n",
    "(http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture06-rnnlm.pdf)\n",
    "![Snip20191021_9.png](https://i.loli.net/2019/10/21/taiU9kjnShBPQs4.png)\n",
    "![Snip20191021_14.png](https://i.loli.net/2019/10/21/fvZ5hFW2Q8xOS9X.png)\n",
    "http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture07-fancy-rnn.pdf\n",
    "![1561874306917-fd4b7f4b-c949-4b2a-b4a8-ab040cf28b87.jpeg](https://i.loli.net/2019/10/21/1FNWJzrdcTSUubf.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果没有安装 keras 和 tensorflow 库\n",
    "#### 请使用 pip install keras tensorflow 安装\n",
    "#### 如果使用conda虚拟环境\n",
    "#### 请使用conda install -c conda-forge keras\n",
    "#### conda install -c conda-forge tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import jieba\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import InputLayer, Embedding, LSTM, Dense, TimeDistributed, SimpleRNN\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_vocab函数的作用是对输入的text列表建立词典vocab和vocab_inv\n",
    "#### 其中vocab是单词到编号的映射\n",
    "#### vocab_inv是编号到单词的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text, vocab_lim):\n",
    "    # Counter工具可以对列表对象进行计数\n",
    "    word_cnt = Counter(text)\n",
    "    # most_common(vocab_lim)可以返回一列(单词，次数)的元组\n",
    "    # x[0]就是把单词从元组里取出来\n",
    "    vocab_inv = [x[0] for x in word_cnt.most_common(vocab_lim)]\n",
    "    # 有了vocab_inv以后，我们可以建立它的逆映射\n",
    "    vocab = {x: index for index, x in enumerate(vocab_inv)}\n",
    "    return vocab, vocab_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'd': 1, 'b': 2, 'c': 3, 'e': 4}\n",
      "['a', 'd', 'b', 'c', 'e']\n"
     ]
    }
   ],
   "source": [
    "vocab, vocab_inv = build_vocab(['a', 'a', 'b', 'c', 'a', 'd', 'd', 'e'], 10)\n",
    "print(vocab)\n",
    "print(vocab_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process_file的作用是读入路径为file_name的文件，并根据use_char_based_model来选择对文件内容的切割方式\n",
    "#### 如果use_char_based_model = True，那么返回的列表是将原文件内容按字分开\n",
    "#### 如果use_char_based_model = False, 那么返回的列表是将原文件中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_name, use_char_based_model):\n",
    "    raw_text = []\n",
    "    # 打开文件\n",
    "    with open(file_name, \"r\") as f:\n",
    "        # 遍历文件中的每一行\n",
    "        for line in f:\n",
    "            if (use_char_based_model):\n",
    "                # 分字\n",
    "                raw_text.extend([str(ch) for ch in line])\n",
    "            else:\n",
    "                # 分词\n",
    "                raw_text.extend([word for word in jieba.cut(line)])\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_matrix用于构建训练数据，输入文本text和词典vocab，length是一对数据的长度，step表示每隔step个位置取一对数据\n",
    "#### 训练数据是一个输入输出对，根据我们对RNN语言模型的理解，输入是$(w_{i}, w_{i+1}, \\dots, w_{i+length})$的话，输出是$(w_{i+1}, w_{i+2}, \\dots, w_{i+length+1})$\n",
    "![](https://cs.stanford.edu/people/karpathy/recurrentjs/eg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(text, vocab, length, step):\n",
    "    M = []\n",
    "    # 将字符数组text转化为对应的编号数组M\n",
    "    for word in text:\n",
    "        # 得到word的编号\n",
    "        index = vocab.get(word)\n",
    "        if (index is None):\n",
    "            # 如果word不在词典里，统一编号为vocab的长度，比如vocab原来有4000个单词，它们标号为0~3999，4000是给不在词典里的词统一预留的编号\n",
    "            M.append(len(vocab))\n",
    "        else:\n",
    "            # 否则就取词典里的编号\n",
    "            M.append(index)\n",
    "    num_sentences = len(M) // length\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    # 从0开始，每隔step个位置取一对数据\n",
    "    for i in range(0, len(M) - length, step):\n",
    "        # [M_{i}, M_{i+1}, ..., M_{i+length}]是输入\n",
    "        X.append(M[i : i + length])\n",
    "        # [M_{i+1}, M_{i+2}, ..., M_{i+length+1}]是输出\n",
    "        Y.append([[x] for x in M[i + 1 : i + length + 1]])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 4000\n",
      "(85698, 11)\n",
      "(85698, 11, 1)\n",
      "[ 42 168 360 935 567   0  12 369 110  65  80]\n",
      "[[168]\n",
      " [360]\n",
      " [935]\n",
      " [567]\n",
      " [  0]\n",
      " [ 12]\n",
      " [369]\n",
      " [110]\n",
      " [ 65]\n",
      " [ 80]\n",
      " [  1]]\n"
     ]
    }
   ],
   "source": [
    "# 设置每对数据的长度为11\n",
    "seq_length = 11\n",
    "# 得到文本分割后的序列，我们采用分字模型\n",
    "raw_text = process_file(\"../input/poetry.txt\", True)\n",
    "# 建立词典，词频排名4000名以后的词不计入词典\n",
    "vocab, vocab_inv = build_vocab(raw_text, 4000)\n",
    "print(len(vocab), len(vocab_inv))\n",
    "# 构建训练数据\n",
    "X, Y = build_matrix(raw_text, vocab, seq_length, seq_length)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 256)         1024256   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, None, 256)         131328    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 4001)        1028257   \n",
      "=================================================================\n",
      "Total params: 2,183,841\n",
      "Trainable params: 2,183,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 搭建模型\n",
    "model = Sequential()\n",
    "# 输入层 设定形状为(None，)可以使模型接受变长输入\n",
    "# 注意到我们在构建训练数据的时候让每次输入的长度都是seq_length，是因为我们要用batch训练的方式来提高并行度\n",
    "# 同样长度的数据可以在一次运算中完成，不同长度的只能逐次计算\n",
    "# 这一层输出的形状是(seq_length, )\n",
    "model.add(InputLayer(input_shape=(None, )))\n",
    "# 词嵌入层，input_dim表示词典的大小，大小4000的词典一共有0~4000个标号的词，每种标号对应一个output_dim维的词向量，设置trainable=True使得词向量矩阵可以调整\n",
    "# 这一层的输出形状是(seq_length, 256, )\n",
    "model.add(Embedding(input_dim=len(vocab) + 1, output_dim=256, trainable=True))\n",
    "# RNN层，输出形状是(seq_length, 256, )\n",
    "model.add(SimpleRNN(units=256, return_sequences=True))\n",
    "# 每个时刻输出的256维向量需要经过一次线性变换（也就是Dense层）转化为4001维的向量，用softmax变换转化为一个4001维的概率概率分布，第i维表示下一个时刻的词是i号单词的概率\n",
    "# Dense即线性变换，TimeDistributed表示对每个时刻都如此作用\n",
    "# 这一层的输出形状是(seq_length, len(vocab) + 1)\n",
    "model.add(TimeDistributed(Dense(units=len(vocab) + 1, activation='softmax')))\n",
    "\n",
    "# 定义loss函数为交叉熵，优化器为Adam，学习率为0.01\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77128 samples, validate on 8570 samples\n",
      "Epoch 1/18\n",
      "77128/77128 [==============================] - 4s 51us/step - loss: 6.3816 - val_loss: 6.0000\n",
      "Epoch 2/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 5.7861 - val_loss: 5.5775\n",
      "Epoch 3/18\n",
      "77128/77128 [==============================] - 3s 43us/step - loss: 5.4058 - val_loss: 5.3178\n",
      "Epoch 4/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 5.2146 - val_loss: 5.2097\n",
      "Epoch 5/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 5.0957 - val_loss: 5.1119\n",
      "Epoch 6/18\n",
      "77128/77128 [==============================] - 3s 43us/step - loss: 4.9761 - val_loss: 5.0260\n",
      "Epoch 7/18\n",
      "77128/77128 [==============================] - 3s 43us/step - loss: 4.8733 - val_loss: 4.9607\n",
      "Epoch 8/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 4.7859 - val_loss: 4.9087\n",
      "Epoch 9/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 4.7093 - val_loss: 4.8694\n",
      "Epoch 10/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 4.6386 - val_loss: 4.8396\n",
      "Epoch 11/18\n",
      "77128/77128 [==============================] - 3s 43us/step - loss: 4.5734 - val_loss: 4.8069\n",
      "Epoch 12/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 4.5120 - val_loss: 4.7847\n",
      "Epoch 13/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 4.4555 - val_loss: 4.7608\n",
      "Epoch 14/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 4.4020 - val_loss: 4.7458\n",
      "Epoch 15/18\n",
      "77128/77128 [==============================] - 3s 43us/step - loss: 4.3521 - val_loss: 4.7336\n",
      "Epoch 16/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 4.3044 - val_loss: 4.7256\n",
      "Epoch 17/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 4.2606 - val_loss: 4.7171\n",
      "Epoch 18/18\n",
      "77128/77128 [==============================] - 3s 42us/step - loss: 4.2187 - val_loss: 4.7167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fed7111ecf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "model.fit(X, Y, batch_size=512, epochs=18, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明月松间照，秋来雪里春。\n",
      "一年一相见，况乃长天路。\n",
      "一旦暮云里，何人无路难。\n",
      "一日无心事，何时见我情。\n",
      "不见青山客，相随日月中。\n",
      "不知人不得，犹在故人家。\n",
      "一叶风光晚，秋风叶未生。\n",
      "不堪秋草树，时向白衣生。\n",
      "白日无尘起，江湖有意多。\n",
      "秋山满江水，月影入秋塘。\n",
      "草色生秋草，秋深不到山。\n",
      "山河一半月，风雨夜中秋。\n",
      "草木秋山近，林花雨后新。\n",
      "春光不相送，春去一沾巾。\n",
      "一日风前月，空门月上楼。\n",
      "不知人不得，不得更无"
     ]
    }
   ],
   "source": [
    "st = '明月松间照'\n",
    "print(st, end='')\n",
    "\n",
    "vocab_inv.append(' ')\n",
    "\n",
    "for i in range(200):\n",
    "    X_sample = np.array([[vocab.get(x, len(vocab)) for x in st]])\n",
    "    pdt = (-model.predict(X_sample)[:, -1: , :][0][0]).argsort()[:4]\n",
    "    if vocab_inv[pdt[0]] == '\\n' or vocab_inv[pdt[0]] == '，' or vocab_inv[pdt[0]] == '。':\n",
    "        ch = vocab_inv[pdt[0]]\n",
    "    else:\n",
    "        ch = vocab_inv[np.random.choice(pdt)]\n",
    "    print(ch, end='')\n",
    "    st = st + ch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
