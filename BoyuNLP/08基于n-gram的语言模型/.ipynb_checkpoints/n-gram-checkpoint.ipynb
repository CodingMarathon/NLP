{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram\n",
    "===\n",
    "\n",
    "统计ngram，并进行简单的文本生成\n",
    "\n",
    "![Snip20191021_12.png](https://i.loli.net/2019/10/21/bcq7uEPMCei1F5I.png)\n",
    "![Snip20191021_13.png](https://i.loli.net/2019/10/21/q2USxHT3wLQiAOZ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.651 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "corpus_file = open(\"../input/santi.txt\", \"r\", encoding=\"utf-8\")\n",
    "raw_text = corpus_file.readlines()\n",
    "text = \"\"\n",
    "for line in raw_text:\n",
    "    text += line.strip()\n",
    "corpus_file.close()\n",
    "# 选择分词或者是分字\n",
    "split_mode = \"jieba\"\n",
    "if split_mode == \"char\":\n",
    "    token_list = [char for char in text]\n",
    "# 利用jieba库分词\n",
    "elif split_mode == \"jieba\":\n",
    "    token_list = [word for word in jieba.cut(text)]\n",
    "# 确定ngram的历史检索长度，即n\n",
    "ngram_len = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化ngram词典\n",
    "ngram_dict = {}\n",
    "for i in range(1, ngram_len): # i = 1 2 3\n",
    "    for j in range(len(token_list) - i - 1):\n",
    "        # 统计前缀是[j, j+i]个词的时候第j+i+1个词出现的次数\n",
    "        key = \"\".join(token_list[j: j + i + 1])\n",
    "        value = \"\".join(token_list[j + i + 1])\n",
    "        # 为第一次出现的键建立字典\n",
    "        if key not in ngram_dict:\n",
    "            ngram_dict[key] = {}\n",
    "        # 初始化字典内每个键值对映射的计数器\n",
    "        if value not in ngram_dict[key]:\n",
    "            ngram_dict[key][value] = 0\n",
    "        ngram_dict[key][value] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程心觉得这个男人是最后一代像男人的声音，她的目光，并向人类世界的命运就像我们这样处于婴儿时代做过的事：在海峡对面的城市。在这紫色的世界中的人们都认为，人类社会的文化艺术与程心想象中的完全不同。在这个时代，他是在三体星系的黑暗森林威慑的特点是：“我们走。”关一帆说。程心看到，不过是一个完整的产业链，动力十足；而她一直在专心干活。程心和关一帆又看到，不过她知道他的大脑。”程心问。“我知道这人不修边幅，头发梳得整齐有形。这个人可以在太空的险恶做了。程心和AA没有回答，没有任何感觉。这时，一个人。“我们要回家了！”程心和AA在病房中为她放了一部全息电影，说他中了好一会儿。是，请宽怒一个卑微画师的冒犯。我是墓地\n"
     ]
    }
   ],
   "source": [
    "# 对输入进行分字或分词\n",
    "start_text = \"程心觉得\"\n",
    "gen_len = 200\n",
    "topn = 3\n",
    "\n",
    "if split_mode == \"char\":\n",
    "    word_list = [char for char in start_text]\n",
    "elif split_mode == \"jieba\":\n",
    "    word_list = [word for word in jieba.cut(start_text)]\n",
    "\n",
    "# gen_len是我们期望的生成字数或词数\n",
    "for i in range(gen_len):\n",
    "    temp_list = []\n",
    "    # 统计给定前小于等于n-1个词的情况下，下一个词的词频分布\n",
    "    for j in range(1, ngram_len):\n",
    "        if j >= len(word_list):\n",
    "            continue\n",
    "        prefix = \"\".join(word_list[-(j + 1):])\n",
    "        if prefix in ngram_dict:\n",
    "            temp_list.extend(ngram_dict[prefix].items())\n",
    "    # 按词频对词排序\n",
    "    temp_list = sorted(temp_list, key=lambda d: d[1], reverse=True)\n",
    "    next_word = random.choice(temp_list[:topn])[0]\n",
    "    word_list.append(next_word)\n",
    "   \n",
    "print(\"\".join(word_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
