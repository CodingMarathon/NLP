{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Word2vec and GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import bz2\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk # standard preprocessing\n",
    "import operator # sorting items in dictionary by value\n",
    "#nltk.download() #tokenizers/punkt/PY3/english.pickle\n",
    "from math import ceil\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This code downloads a [dataset](http://www.evanjones.ca/software/wikipedia2text.html) consisting of several Wikipedia articles totaling up to roughly 61 megabytes. Additionally the code makes sure the file has the correct size after downloading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified wikipedia2text-extracted.txt.bz2\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.evanjones.ca/software/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('wikipedia2text-extracted.txt.bz2', 18377035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data with Preprocessing with NLTK\n",
    "Reads data as it is to a string, convert to lower-case and tokenize it using the nltk library. This code reads data in 1MB portions as processing the full text at once slows down the task and returns a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data size 3361213\n",
      "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"\n",
    "  Extract the first file enclosed in a zip file as a list of words\n",
    "  and pre-processes it using the nltk python library\n",
    "  \"\"\"\n",
    "\n",
    "  with bz2.BZ2File(filename) as f:\n",
    "\n",
    "    data = []\n",
    "    file_size = os.stat(filename).st_size\n",
    "    chunk_size = 1024 * 1024 # reading 1 MB at a time as the dataset is moderately large\n",
    "    print('Reading data...')\n",
    "    for i in range(ceil(file_size//chunk_size)+1):\n",
    "        bytes_to_read = min(chunk_size,file_size-(i*chunk_size))\n",
    "        file_string = f.read(bytes_to_read).decode('utf-8')\n",
    "        file_string = file_string.lower()\n",
    "        # tokenizes a string to words residing in a list\n",
    "        file_string = nltk.word_tokenize(file_string)\n",
    "        data.extend(file_string)\n",
    "  return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))\n",
    "token_count = len(words)\n",
    "\n",
    "print('Example words (start): ',words[:10])\n",
    "print('Example words (end): ',words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "\n",
    "* `dictionary`: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 68747], ('the', 226893), (',', 184013), ('.', 120919), ('of', 116323)]\n",
      "Sample data [1721, 9, 8, 16479, 223, 4, 5168, 4459, 26, 11597]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  assert len(dictionary) == vocabulary_size\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed']\n",
      "\n",
      "with window_size = 1:\n",
      "    batch: ['is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at']\n",
      "    labels: [['propaganda', 'a'], ['is', 'concerted'], ['a', 'set'], ['concerted', 'of'], ['set', 'messages'], ['of', 'aimed'], ['messages', 'at'], ['aimed', 'influencing']]\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: ['a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "    labels: [['propaganda', 'is', 'concerted', 'set'], ['is', 'a', 'set', 'of'], ['a', 'concerted', 'of', 'messages'], ['concerted', 'set', 'messages', 'aimed'], ['set', 'of', 'aimed', 'at'], ['of', 'messages', 'at', 'influencing'], ['messages', 'aimed', 'influencing', 'the'], ['aimed', 'at', 'the', 'opinions']]\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, window_size):\n",
    "  global data_index\n",
    "\n",
    "  # two numpy arras to hold target words (batch)\n",
    "  # and context words (labels)\n",
    "  # Note that the labels array has 2*window_size columns\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 2*window_size), dtype=np.int32)\n",
    "  \n",
    "  # span defines the total window size, where\n",
    "  # data we consider at an instance looks as follows. \n",
    "  # [ skip_window target skip_window ]\n",
    "  span = 2 * window_size + 1 # [ skip_window target skip_window ]\n",
    "  \n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  \n",
    "  # Fill the buffer and update the data_index\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  \n",
    "  # for a full length of batch size, we do the following\n",
    "  # make the target word the i th input word (i th row of batch)\n",
    "  # make all the context words the columns of labels array\n",
    "  # Update the data index and the buffer \n",
    "  for i in range(batch_size):\n",
    "    batch[i] = buffer[window_size]\n",
    "    labels[i, :] = [buffer[span_idx] for span_idx in list(range(0,window_size))+ list(range(window_size+1,span))]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  \n",
    "  return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for window_size in [1,2]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, window_size=window_size)\n",
    "    print('\\nwith window_size = %d:' % window_size)\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [[reverse_dictionary[li] for li in lbls] for lbls in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Skip-Gram Algorithm\n",
    "The basic idea behind the structured skip-gram algorithm is to pay attention to the position of the context words during learning. Giving the algorithm the power to distinguish between words falling very close to the target word and the ones that fall far away from the context words allow the structured skip-gram model to learn better word vectors ([Paper](http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf)). You can learn about this algorithm in more detail in Chapter 4 text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters\n",
    "\n",
    "Here we define several hyperparameters including `batch_size` (amount of samples in a single batch) `embedding_size` (size of embedding vectors) `window_size` (context window size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # Data points in a single batch\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "window_size = 2 # How many words to consider left and right.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 50\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
    "\n",
    "num_sampled = 32 # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Inputs and Outputs\n",
    "\n",
    "Here we define placeholders for feeding in training inputs and outputs (each of size `batch_size`) and a constant tensor to contain validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training input data (target word IDs).\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# Training input label data (context word IDs)\n",
    "train_labels = [tf.placeholder(tf.int32, shape=[batch_size, 1]) for _ in range(2*window_size)]\n",
    "# Validation input data, we don't need a placeholder\n",
    "# as we have already defined the IDs of the words selected\n",
    "# as validation data\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model Parameters and Other Variables\n",
    "We now define several TensorFlow variables such as an embedding layer (`embeddings`) and neural network parameters (`softmax_weights` and `softmax_biases`). Note that the softmax weights is `2*window_size` larger than the original skip-gram algorithms's softmax weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(\n",
    "tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "softmax_weights = [tf.Variable(\n",
    "tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                     stddev=0.5 / math.sqrt(embedding_size))) for _ in range(2*window_size)]\n",
    "softmax_biases = [tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01)) for _ in range(2*window_size)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Computations\n",
    "\n",
    "We first defing a lookup function to fetch the corresponding embedding vectors for a set of given inputs. With that, we define negative sampling loss function `tf.nn.sampled_softmax_loss` which takes in the embedding vectors and previously defined neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model.\n",
    "# Look up embeddings for inputs.\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "\n",
    "# You might see the warning when running the line below\n",
    "# WARNING:tensorflow:From c:\\...\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1346: \n",
    "#softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and \n",
    "# will be removed in a future version.\n",
    "# This is due to the sampled_softmax_loss function using a deprecated function internally\n",
    "# therefore, this is not an error in the code and you can ignore this error\n",
    "\n",
    "# Compute the softmax loss, using a sample of the negative labels each time.\n",
    "loss = tf.reduce_sum(\n",
    "[\n",
    "    tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights[wi], biases=softmax_biases[wi], inputs=embed,\n",
    "                           labels=train_labels[wi], num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "    for wi in range(window_size*2)\n",
    "]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Word Similarities \n",
    "We calculate the similarity between two given words in terms of the cosine distance. To do this efficiently we use matrix operations to do so, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity between minibatch examples and all embeddings.\n",
    "# We use the cosine distance:\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(\n",
    "normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter Optimizer\n",
    "\n",
    "We then define a constant learning rate and an optimizer which uses the Adagrad method. Feel free to experiment with other optimizers listed [here](https://www.tensorflow.org/api_guides/python/train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tensorflow1.15-cpu\\lib\\site-packages\\tensorflow_core\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Structured Skip-gram Algorithm\n",
    "\n",
    "Here we run the structured skip-gram algorithm we defined above. Specifically, we first initialize variables, and then train the algorithm for many steps (`num_steps`). And every few steps we evaluate the algorithm on a fixed validation set and print out the words that appear to be closest for a given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 2000: 14.841298\n",
      "Average loss at step 4000: 12.946791\n",
      "Average loss at step 6000: 12.501016\n",
      "Average loss at step 8000: 12.185172\n",
      "Average loss at step 10000: 12.003610\n",
      "Nearest to is: was, are, has, became, can, believes, includes, interscope,\n",
      "Nearest to other: many, numerous, these, different, several, modern, some, most,\n",
      "Nearest to has: had, have, was, is, also, can, includes, citizenry,\n",
      "Nearest to its: their, his, the, pedestrian-only, special, monophthongs, rongji, an,\n",
      "Nearest to his: their, her, its, the, s, these, legal, 's,\n",
      "Nearest to .: ;, ,, of, :, and, bromwich, .., skies,\n",
      "Nearest to he: she, they, it, who, there, had, was, i,\n",
      "Nearest to which: that, but, where, what, who, and, it, they,\n",
      "Nearest to as: by, paganini, tlatoani, inhuman, hgh, babel, and, in,\n",
      "Nearest to not: n't, still, also, often, never, now, would, always,\n",
      "Nearest to first: second, last, only, largest, most, final, best, greatest,\n",
      "Nearest to on: through, affecting, decorous, vehicle-to-grid, along, across, nests, over,\n",
      "Nearest to city: country, state, island, area, 1950s, tripoli, community, issyk,\n",
      "Nearest to or: and, (, hh, half-hour, mate, insurrectionists, color-coded, 'you,\n",
      "Nearest to to: would, can, will, by, not, for, three-and-a-half, defended,\n",
      "Nearest to ``: noting, “, sandbar, ecliptic, extracellular, getting, orthodoxy, starbucks,\n",
      "Average loss at step 12000: 11.888670\n",
      "Average loss at step 14000: 11.722114\n",
      "Average loss at step 16000: 11.672461\n",
      "Average loss at step 18000: 11.604404\n",
      "Average loss at step 20000: 11.574999\n",
      "Nearest to is: was, are, has, became, were, includes, being, can,\n",
      "Nearest to other: many, numerous, various, several, some, most, different, these,\n",
      "Nearest to has: had, have, is, includes, was, having, also, contains,\n",
      "Nearest to its: their, his, the, her, pedestrian-only, a, monophthongs, lambrigg,\n",
      "Nearest to his: her, their, its, the, my, s, tiananmen, these,\n",
      "Nearest to .: ;, ,, :, (, psalm, skies, -, and,\n",
      "Nearest to he: she, they, it, who, ford, exorcism, there, hitler,\n",
      "Nearest to which: that, who, where, but, what, whom, they, punish,\n",
      "Nearest to as: hyderabad, tlatoani, levitt, kalākaua, swings, brokered, glove, including,\n",
      "Nearest to not: n't, still, often, also, usually, generally, never, sometimes,\n",
      "Nearest to first: last, second, next, only, final, largest, earliest, most,\n",
      "Nearest to on: upon, in, affecting, dated, gripped, decorous, vehicle-to-grid, with,\n",
      "Nearest to city: country, island, state, town, nation, area, region, landvetter,\n",
      "Nearest to or: and, insurrectionists, well-represented, (, without, including, per, extracting,\n",
      "Nearest to to: will, would, can, could, toward, afrikaner-dutch, three-and-a-half, mardi,\n",
      "Nearest to ``: “, extracellular, obey, excess, |oct_precip_mm, ecliptic, permanent, noting,\n",
      "Average loss at step 22000: 11.469266\n",
      "Average loss at step 24000: 11.451837\n",
      "Average loss at step 26000: 11.297572\n",
      "Average loss at step 28000: 10.895111\n",
      "Average loss at step 30000: 10.735077\n",
      "Nearest to is: was, has, became, are, salvador, remains, legate, includes,\n",
      "Nearest to other: various, numerous, many, several, different, modern, major, twisting,\n",
      "Nearest to has: had, have, is, includes, was, having, contains, malaysian,\n",
      "Nearest to its: their, his, the, her, monophthongs, lazarus, whose, importing,\n",
      "Nearest to his: her, their, its, my, 's, municipios, french-dominated, tiananmen,\n",
      "Nearest to .: ;, ,, non-serbs, psalm, interpreting, bromwich, tabloids, attire,\n",
      "Nearest to he: she, they, it, who, churchill, we, hitler, napoleon,\n",
      "Nearest to which: whom, where, but, that, what, and, who, fueling,\n",
      "Nearest to as: conservationist, paganini, x-rays, including, fielders, brokered, tlatoani, ucd,\n",
      "Nearest to not: n't, never, still, usually, also, often, actually, probably,\n",
      "Nearest to first: last, second, next, fourth, earliest, largest, greatest, final,\n",
      "Nearest to on: affecting, dated, upon, across, gripped, along, vehicle-to-grid, restrict,\n",
      "Nearest to city: country, town, state, cities, island, nation, region, canal,\n",
      "Nearest to or: and, hillbilly, organizational, extravagant, kcbs, jesu, extracting, resistive,\n",
      "Nearest to to: will, three-and-a-half, euthyphro, maddrey, helped, microscopy, countercultural, amalric,\n",
      "Nearest to ``: “, sandbar, 1-3, quadruped, amenable, ruined, |oct_precip_mm, 735,\n",
      "Average loss at step 32000: 10.750528\n",
      "Average loss at step 34000: 10.743460\n",
      "Average loss at step 36000: 10.702598\n",
      "Average loss at step 38000: 10.605117\n",
      "Average loss at step 40000: 10.661873\n",
      "Nearest to is: was, are, has, becomes, hindering, salvador, became, contains,\n",
      "Nearest to other: various, numerous, many, several, different, individual, others, these,\n",
      "Nearest to has: had, have, is, was, having, contains, includes, receives,\n",
      "Nearest to its: their, his, pedestrian-only, flaming, her, whose, cladistic, amartya,\n",
      "Nearest to his: her, their, its, my, the, himself, amadou, french-dominated,\n",
      "Nearest to .: ;, ,, non-serbs, :, .., porsche, tabloids, explores,\n",
      "Nearest to he: she, they, it, tutu, who, churchill, we, napoleon,\n",
      "Nearest to which: that, what, whom, who, where, but, whose, fueling,\n",
      "Nearest to as: brokered, fielders, swings, hyderabad, depleted, earthenware, glove, sow,\n",
      "Nearest to not: n't, still, never, usually, always, often, generally, now,\n",
      "Nearest to first: second, last, next, fourth, earliest, oldest, final, largest,\n",
      "Nearest to on: dated, affecting, upon, dhahran, vehicle-to-grid, gripped, zaanstad, devastate,\n",
      "Nearest to city: town, country, nation, cities, island, empires, state, landvetter,\n",
      "Nearest to or: and, hillbilly, extravagant, organizational, insurrectionists, habeas, starchy, half-hour,\n",
      "Nearest to to: would, mello, must, resuming, three-and-a-half, afrikaner-dutch, will, countercultural,\n",
      "Nearest to ``: “, noting, |oct_precip_mm, starbucks, extracellular, obey, 5.4, getting,\n",
      "Average loss at step 42000: 10.595923\n",
      "Average loss at step 44000: 10.667019\n",
      "Average loss at step 46000: 10.643347\n",
      "Average loss at step 48000: 10.598962\n",
      "Average loss at step 50000: 10.681114\n",
      "Nearest to is: was, are, has, involves, became, remains, mortars, becomes,\n",
      "Nearest to other: many, various, numerous, several, others, different, these, even,\n",
      "Nearest to has: had, have, having, is, contains, includes, possesses, receives,\n",
      "Nearest to its: their, his, the, importing, pedestrian-only, cuba, flaming, attachments,\n",
      "Nearest to his: her, their, its, my, wrongful, mccauley, municipios, amadou,\n",
      "Nearest to .: ;, ,, :, of, .., ?, potawatomi, euphrates,\n",
      "Nearest to he: she, they, it, churchill, tutu, i, jefferson, hitler,\n",
      "Nearest to which: that, where, whom, what, and, but, who, whose,\n",
      "Nearest to as: swings, hyderabad, zhang, depleted, paganini, stocked, result, bacalhau,\n",
      "Nearest to not: n't, never, still, also, now, probably, usually, always,\n",
      "Nearest to first: last, second, next, fourth, earliest, final, latter, oldest,\n",
      "Nearest to on: upon, dated, vehicle-to-grid, affecting, caiman, in, gripped, disavowing,\n",
      "Nearest to city: town, country, state, nation, cities, region, willie, landvetter,\n",
      "Nearest to or: and, hillbilly, organizational, habeas, bartok, fenway, meaning, and/or,\n",
      "Nearest to to: three-and-a-half, could, would, will, toward, must, towards, revolutionized,\n",
      "Nearest to ``: “, |oct_precip_mm, ruined, excess, if, literally, noting, submit,\n",
      "Average loss at step 52000: 10.444386\n",
      "Average loss at step 54000: 10.324789\n",
      "Average loss at step 56000: 10.200975\n",
      "Average loss at step 58000: 10.208207\n",
      "Average loss at step 60000: 10.198439\n",
      "Nearest to is: was, are, becomes, became, has, remains, lies, lends,\n",
      "Nearest to other: various, numerous, many, others, several, rearguard, estado, afterburners,\n",
      "Nearest to has: had, have, is, possesses, was, corresponds, holds, includes,\n",
      "Nearest to its: their, his, pedestrian-only, cuba, importing, justify, lazarus, monophthongs,\n",
      "Nearest to his: her, their, my, its, 's, sigmund, wrongful, fourth-largest,\n",
      "Nearest to .: ,, ;, anti-aircraft/anti-missile, amyntas, 6-2., 616, 2002., copulated,\n",
      "Nearest to he: she, they, it, who, we, churchill, tutu, jefferson,\n",
      "Nearest to which: whom, that, what, where, but, who, whose, clustering,\n",
      "Nearest to as: brokered, tlatoani, fielders, glove, clothed, discovers, firearm, earthenware,\n",
      "Nearest to not: n't, never, still, now, probably, hardly, also, always,\n",
      "Nearest to first: last, second, earliest, fourth, initial, oldest, latter, next,\n",
      "Nearest to on: upon, affecting, decorous, gripped, dhahran, vehicle-to-grid, revealing, 1138,\n",
      "Nearest to city: town, country, nation, cities, island, state, region, area,\n",
      "Nearest to or: and, hillbilly, half-hour, insurrectionists, ink, jesu, meaning, mate,\n",
      "Nearest to to: revolutionized, must, condoned, three-and-a-half, would, confuse, should, necessarily,\n",
      "Nearest to ``: “, 2024, 759, really, ruined, lukes, ecliptic, sandbar,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 62000: 10.199070\n",
      "Average loss at step 64000: 10.101593\n",
      "Average loss at step 66000: 10.192547\n",
      "Average loss at step 68000: 10.158808\n",
      "Average loss at step 70000: 10.166063\n",
      "Nearest to is: was, are, has, becomes, mortars, denouncing, became, salvador,\n",
      "Nearest to other: various, many, others, numerous, several, whip, earliest, lacks,\n",
      "Nearest to has: had, have, is, includes, contains, having, holds, possesses,\n",
      "Nearest to its: their, his, amartya, whose, pedestrian-only, monophthongs, lazarus, her,\n",
      "Nearest to his: her, their, its, my, 's, amadou, french-dominated, feminist,\n",
      "Nearest to .: ;, ,, of, acquires, carcass, —, reopened, :,\n",
      "Nearest to he: she, they, it, tutu, who, borges, rachmaninoff, koniecpolski,\n",
      "Nearest to which: that, whom, what, who, where, and, punish, whose,\n",
      "Nearest to as: swings, ||-||, tlatoani, brokered, levitt, 640, glove, bacalhau,\n",
      "Nearest to not: n't, never, now, still, also, always, usually, probably,\n",
      "Nearest to first: last, second, earliest, next, fourth, oldest, initial, largest,\n",
      "Nearest to on: dated, upon, affecting, gripped, decorous, dhahran, in, 1138,\n",
      "Nearest to city: town, country, nation, metropolitan, cities, mpla, county, state,\n",
      "Nearest to or: and, 67.2, and/or, insurrectionists, fortification, hillbilly, mate, bartok,\n",
      "Nearest to to: would, three-and-a-half, mello, should, recreate, afrikaner-dutch, for, km³,\n",
      "Nearest to ``: “, ecliptic, amenable, obey, extracellular, |oct_precip_mm, getting, literally,\n",
      "Average loss at step 72000: 10.207219\n",
      "Average loss at step 74000: 10.140642\n",
      "Average loss at step 76000: 10.252022\n",
      "Average loss at step 78000: 10.018718\n",
      "Average loss at step 80000: 9.881888\n",
      "Nearest to is: was, are, lies, has, becomes, remains, salvador, provides,\n",
      "Nearest to other: various, many, numerous, others, several, rearguard, previous, inuit,\n",
      "Nearest to has: had, have, is, contains, having, hosts, possesses, holds,\n",
      "Nearest to its: their, his, importing, whose, monophthongs, flaming, lazarus, vojvodina,\n",
      "Nearest to his: her, their, my, its, 's, french-dominated, wrongful, tiananmen,\n",
      "Nearest to .: ;, ,, residing, kohl, non-serbs, skies, 1937., :,\n",
      "Nearest to he: she, they, it, we, tutu, churchill, marx, saladin,\n",
      "Nearest to which: whom, that, where, what, who, but, tc, 1077th,\n",
      "Nearest to as: brokered, tlatoani, perfumes, babel, utilising, deepening, conservationist, ||-||,\n",
      "Nearest to not: n't, never, still, also, hardly, exceptionally, now, probably,\n",
      "Nearest to first: last, second, fourth, next, earliest, third, fifth, oldest,\n",
      "Nearest to on: dated, upon, gripped, affecting, divorcing, dhahran, cookies, reads,\n",
      "Nearest to city: town, country, nation, cities, mpla, state, metropolitan, detroit,\n",
      "Nearest to or: hillbilly, and, alamos, jesu, frustration, minus, ink, constitutive,\n",
      "Nearest to to: euthyphro, towards, will, toward, three-and-a-half, abusing, microscopy, reached,\n",
      "Nearest to ``: “, really, ruined, ecliptic, quadruped, 735, amenable, boulé,\n",
      "Average loss at step 82000: 9.912069\n",
      "Average loss at step 84000: 9.876417\n",
      "Average loss at step 86000: 9.896000\n",
      "Average loss at step 88000: 9.827127\n",
      "Average loss at step 90000: 9.798252\n",
      "Nearest to is: was, are, has, becomes, hindering, contains, lies, comes,\n",
      "Nearest to other: various, others, many, numerous, rearguard, different, individual, 142,\n",
      "Nearest to has: had, have, contains, is, holds, receives, possesses, having,\n",
      "Nearest to its: their, his, pedestrian-only, the, flaming, gaining, lazarus, cladistic,\n",
      "Nearest to his: her, their, its, my, feminist, amadou, 1400, french-dominated,\n",
      "Nearest to .: ;, ,, .., non-empirical, wilderness, minding, squarely, overlooks,\n",
      "Nearest to he: she, they, it, tutu, who, we, rachmaninoff, rawls,\n",
      "Nearest to which: that, whom, who, what, where, but, whose, and,\n",
      "Nearest to as: levitt, tlatoani, earthenware, bacalhau, perfumes, furthered, reverend, brigitte,\n",
      "Nearest to not: n't, never, still, moderately, hardly, always, now, also,\n",
      "Nearest to first: last, second, earliest, fourth, oldest, next, fifth, initial,\n",
      "Nearest to on: dated, vehicle-to-grid, affecting, gripped, dhahran, upon, devastate, phantom,\n",
      "Nearest to city: town, country, cities, nation, village, mpla, atlanta, purges,\n",
      "Nearest to or: and, insurrectionists, rollberg, ink, bartok, hillbilly, mathilde, sidgwick,\n",
      "Nearest to to: afrikaner-dutch, euthyphro, three-and-a-half, upstream, would, resuming, should, validated,\n",
      "Nearest to ``: “, hellene, musée, really, ecliptic, 735, starbucks, ‘,\n",
      "Average loss at step 92000: 9.873334\n",
      "Average loss at step 94000: 9.851433\n",
      "Average loss at step 96000: 9.916377\n",
      "Average loss at step 98000: 9.863780\n",
      "Average loss at step 100000: 9.907088\n",
      "Nearest to is: was, has, are, becomes, represents, became, lies, remains,\n",
      "Nearest to other: many, various, numerous, others, several, neighbouring, inuit, previous,\n",
      "Nearest to has: had, have, is, holds, contains, possesses, having, receives,\n",
      "Nearest to its: their, flaming, pedestrian-only, his, zacatecas, amartya, importing, monophthongs,\n",
      "Nearest to his: her, their, my, its, wrongful, the, municipios, amadou,\n",
      "Nearest to .: ;, ,, .., corfu, electromagnets, of, tabloids, :,\n",
      "Nearest to he: she, they, it, who, secretly, tutu, tesla, laver,\n",
      "Nearest to which: that, whom, what, where, who, whose, zairian, fueling,\n",
      "Nearest to as: levitt, bacalhau, tlatoani, conservationist, reverend, swings, avery, liga,\n",
      "Nearest to not: n't, never, still, also, merely, now, hardly, moderately,\n",
      "Nearest to first: last, second, earliest, fourth, next, third, fifth, oldest,\n",
      "Nearest to on: dated, upon, gripped, affecting, vehicle-to-grid, usibepu, decorous, caiman,\n",
      "Nearest to city: town, country, nation, cities, state, mpla, baltimore, county,\n",
      "Nearest to or: and, and/or, hillbilly, 67.2, fenway, bartok, kfc, minus,\n",
      "Nearest to to: could, three-and-a-half, would, will, must, mello, afrikaner-dutch, anti-depressant,\n",
      "Nearest to ``: “, |oct_precip_mm, literally, 759, hellene, ecliptic, ruined, ever-changing,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "decay_learning_rate_every = 2000\n",
    "skip_gram_loss = [] # Collect the sequential loss values for plotting purposes\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, window_size)\n",
    "    feed_dict = {train_dataset : batch_data}\n",
    "    for wi in range(2*window_size):\n",
    "        feed_dict.update({train_labels[wi]:np.reshape(batch_labels[:,wi],(-1,1))})\n",
    "    \n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    \n",
    "    if (step+1) % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      skip_gram_loss.append(average_loss)\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if (step+1) % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "  skip_gram_final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "# We will save the word vectors learned and the loss over time\n",
    "# as this information is required later for comparisons\n",
    "np.save('struct_skip_embeddings',skip_gram_final_embeddings)\n",
    "\n",
    "with open('struct_skip_losses.csv', 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(skip_gram_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow1.15-cpu]",
   "language": "python",
   "name": "conda-env-tensorflow1.15-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
