{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Word2vec Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "##from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import bz2\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk # standard preprocessing\n",
    "##import operator # sorting items in dictionary by value\n",
    "#nltk.download() #tokenizers/punkt/PY3/english.pickle\n",
    "from math import ceil\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This code downloads a [dataset](http://www.evanjones.ca/software/wikipedia2text.html) consisting of several Wikipedia articles totaling up to roughly 61 megabytes. Additionally the code makes sure the file has the correct size after downloading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified wikipedia2text-extracted.txt.bz2\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.evanjones.ca/software/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('wikipedia2text-extracted.txt.bz2', 18377035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data with Preprocessing with NLTK\n",
    "Reads data as it is to a string, convert to lower-case and tokenize it using the nltk library. This code reads data in 1MB portions as processing the full text at once slows down the task and returns a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data size 3361213\n",
      "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"\n",
    "  Extract the first file enclosed in a zip file as a list of words\n",
    "  and pre-processes it using the nltk python library\n",
    "  \"\"\"\n",
    "\n",
    "  with bz2.BZ2File(filename) as f:\n",
    "\n",
    "    data = []\n",
    "    file_size = os.stat(filename).st_size\n",
    "    chunk_size = 1024 * 1024 # reading 1 MB at a time as the dataset is moderately large\n",
    "    print('Reading data...')\n",
    "    for i in range(ceil(file_size//chunk_size)+1):\n",
    "        bytes_to_read = min(chunk_size,file_size-(i*chunk_size))\n",
    "        file_string = f.read(bytes_to_read).decode('utf-8')\n",
    "        file_string = file_string.lower()\n",
    "        # tokenizes a string to words residing in a list\n",
    "        file_string = nltk.word_tokenize(file_string)\n",
    "        data.extend(file_string)\n",
    "  return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))\n",
    "token_count = len(words)\n",
    "\n",
    "print('Example words (start): ',words[:10])\n",
    "print('Example words (end): ',words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "\n",
    "* `dictionary`: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 68747], ('the', 226893), (',', 184013), ('.', 120919), ('of', 116323)]\n",
      "Sample data [1721, 9, 8, 16479, 223, 4, 5168, 4459, 26, 11597]\n"
     ]
    }
   ],
   "source": [
    "# we restrict our vocabulary size to 50000\n",
    "vocabulary_size = 50000 \n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  # Gets only the vocabulary_size most common words as the vocabulary\n",
    "  # All the other words will be replaced with UNK token\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "\n",
    "  # Create an ID for each word by giving the current length of the dictionary\n",
    "  # And adding that item to the dictionary\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "    \n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  # Traverse through all the text we have and produce a list\n",
    "  # where each element corresponds to the ID of the word found at that index\n",
    "  for word in words:\n",
    "    # If word is in the dictionary use the word ID,\n",
    "    # else use the ID of the special token \"UNK\"\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "    \n",
    "  # update the count variable with the number of UNK occurences\n",
    "  count[0][1] = unk_count\n",
    "  \n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  # Make sure the dictionary is of size of the vocabulary\n",
    "  assert len(dictionary) == vocabulary_size\n",
    "    \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data for Skip-Gram\n",
    "Generates a batch or target words (`batch`) and a batch of corresponding context words (`labels`). It reads `2*window_size+1` words at a time (called a `span`) and create `2*window_size` datapoints in a single span. The function continue in this manner until `batch_size` datapoints are created. Everytime we reach the end of the word sequence, we start from beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed']\n",
      "\n",
      "with window_size = 1:\n",
      "    batch: ['is', 'is', 'a', 'a', 'concerted', 'concerted', 'set', 'set']\n",
      "    labels: ['propaganda', 'a', 'is', 'concerted', 'a', 'set', 'concerted', 'of']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: ['a', 'a', 'a', 'a', 'concerted', 'concerted', 'concerted', 'concerted']\n",
      "    labels: ['propaganda', 'is', 'concerted', 'set', 'is', 'a', 'set', 'of']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch_skip_gram(batch_size, window_size):\n",
    "  # data_index is updated by 1 everytime we read a data point\n",
    "  global data_index \n",
    "    \n",
    "  # two numpy arras to hold target words (batch)\n",
    "  # and context words (labels)\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "  # span defines the total window size, where\n",
    "  # data we consider at an instance looks as follows. \n",
    "  # [ skip_window target skip_window ]\n",
    "  span = 2 * window_size + 1 \n",
    "    \n",
    "  # The buffer holds the data contained within the span\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  \n",
    "  # Fill the buffer and update the data_index\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  \n",
    "  # This is the number of context words we sample for a single target word\n",
    "  num_samples = 2*window_size \n",
    "\n",
    "  # We break the batch reading into two for loops\n",
    "  # The inner for loop fills in the batch and labels with \n",
    "  # num_samples data points using data contained withing the span\n",
    "  # The outper for loop repeat this for batch_size//num_samples times\n",
    "  # to produce a full batch\n",
    "  for i in range(batch_size // num_samples):\n",
    "    k=0\n",
    "    # avoid the target word itself as a prediction\n",
    "    # fill in batch and label numpy arrays\n",
    "    for j in list(range(window_size))+list(range(window_size+1,2*window_size+1)):\n",
    "      batch[i * num_samples + k] = buffer[window_size]\n",
    "      labels[i * num_samples + k, 0] = buffer[j]\n",
    "      k += 1 \n",
    "    \n",
    "    # Everytime we read num_samples data points,\n",
    "    # we have created the maximum number of datapoints possible\n",
    "    # withing a single span, so we need to move the span by 1\n",
    "    # to create a fresh new span\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for window_size in [1, 2]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch_skip_gram(batch_size=8, window_size=window_size)\n",
    "    print('\\nwith window_size = %d:' % window_size)\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Skip-Gram Algorithm\n",
    "The original skip-gram algorithm did not have a hidden layer but calculated the loss from the embeddings themselves. Therefore, skip-gram algorithm had two different embedding layers one for inputs and one for outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters\n",
    "\n",
    "Here we define several hyperparameters including `batch_size` (amount of samples in a single batch) `embedding_size` (size of embedding vectors) `window_size` (context window size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # Data points in a single batch\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "window_size = 4 # How many words to consider left and right.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 50\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
    "\n",
    "num_sampled = 32 # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Inputs and Outputs\n",
    "\n",
    "Here we define placeholders for feeding in training inputs and outputs (each of size `batch_size`) and a constant tensor to contain validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training input data (target word IDs).\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# Training input label data (context word IDs)\n",
    "train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\n",
    "# Validation input data, we don't need a placeholder\n",
    "# as we have already defined the IDs of the words selected\n",
    "# as validation data\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model Parameters and Other Variables\n",
    "We now define two TensorFlow variables as embedding layers(`in_embeddings` and `out_embeddings`). Note that we do not have any neural network parameters (`softmax_weights` and `softmax_biases`) as we had in the skip-gram algorithm code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "# Embedding layers, contains the word embeddings\n",
    "# We define two embedding layers\n",
    "# in_embeddings is used to lookup embeddings corresponding to target words (inputs)\n",
    "in_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    ")\n",
    "# out_embeddings is used to lookup embeddings corresponding to contect words (labels)\n",
    "out_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Computations\n",
    "\n",
    "Here we define several TensorFlow opeartions required for computing loss and predictions. We first defing an opeartion to fetch negative samples for a given batch of data. Next we define embedding lookup functions for both true (`in_embed` and `out_embed`) and negative (`negative_embed`) data where these opeartions fetch the corresponding embedding vectors for a set of given inputs. With that, we define negative sampling loss manually using the embeddings returned by the lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute negative sampels for a given batch of data\n",
    "# Returns a [num_sampled] size Tensor\n",
    "negative_samples, _, _ = tf.nn.log_uniform_candidate_sampler(train_labels, num_true=1, num_sampled=num_sampled, \n",
    "                                                           unique=True, range_max=vocabulary_size)\n",
    "# 2. Look up embeddings for inputs, outputs and negative samples.\n",
    "in_embed = tf.nn.embedding_lookup(in_embeddings, train_dataset)\n",
    "out_embed = tf.nn.embedding_lookup(out_embeddings, tf.reshape(train_labels,[-1]))\n",
    "negative_embed = tf.nn.embedding_lookup(out_embeddings, negative_samples) \n",
    "\n",
    "# 3. Manually defining negative sample loss\n",
    "# As Tensorflow have a limited amount of flexibility in the built-in sampled_softmax_loss function,\n",
    "# we have to manually define the loss fuction.\n",
    "\n",
    "# 3.1. Computing the loss for the positive sample\n",
    "# Exactly we compute log(sigma(v_o * v_i^T)) with this equation\n",
    "loss = tf.reduce_mean(\n",
    "  tf.log(\n",
    "      tf.nn.sigmoid(\n",
    "          tf.reduce_sum(\n",
    "              tf.diag([1.0 for _ in range(batch_size)])*\n",
    "              tf.matmul(out_embed,tf.transpose(in_embed)),\n",
    "          axis=0)\n",
    "      )\n",
    "  )      \n",
    ")\n",
    "\n",
    "# 3.2. Computing loss for the negative samples\n",
    "# We compute sum(log(sigma(-v_no * v_i^T))) with the following\n",
    "# Note: The exact way this part is computed in TensorFlow library appears to be\n",
    "# by taking only the weights corresponding to true samples and negative samples\n",
    "# and then computing the softmax_cross_entropy_with_logits for that subset of weights.\n",
    "# More infor at: https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/nn_impl.py\n",
    "# Though the approach is different, the idea remains the same\n",
    "loss += tf.reduce_mean(\n",
    "  tf.reduce_sum(\n",
    "      tf.log(tf.nn.sigmoid(-tf.matmul(negative_embed,tf.transpose(in_embed)))),\n",
    "      axis=0\n",
    "  )\n",
    ")\n",
    "\n",
    "# The above is the log likelihood. \n",
    "# We would like to transform this to the negative log likelihood\n",
    "# to convert this to a loss. This provides us with\n",
    "# L = - (log(sigma(v_o * v_i^T))+sum(log(sigma(-v_no * v_i^T))))\n",
    "loss *= -1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Word Similarities \n",
    "We calculate the similarity between two given words in terms of the cosine distance. To do this efficiently we use matrix operations to do so, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity between minibatch examples and all embeddings.\n",
    "# We use the cosine distance:\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square((in_embeddings+out_embeddings)/2.0), 1, keepdims=True))\n",
    "normalized_embeddings = out_embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(\n",
    "normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter Optimizer\n",
    "\n",
    "We then define a constant learning rate and an optimizer which uses the Adagrad method. Feel free to experiment with other optimizers listed [here](https://www.tensorflow.org/api_guides/python/train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tensorflow1.15-cpu\\lib\\site-packages\\tensorflow_core\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Optimizer.\n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Original Skip-gram Algorithm\n",
    "\n",
    "Here we run the original skip-gram algorithm we defined above. Specifically, we first initialize variables, and then train the algorithm for many steps (`num_steps`). And every few steps we evaluate the algorithm on a fixed validation set and print out the words that appear to be closest for a given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 2000: nan\n",
      "Average loss at step 4000: nan\n",
      "Average loss at step 6000: nan\n",
      "Average loss at step 8000: nan\n",
      "Average loss at step 10000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Average loss at step 12000: nan\n",
      "Average loss at step 14000: nan\n",
      "Average loss at step 16000: nan\n",
      "Average loss at step 18000: nan\n",
      "Average loss at step 20000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Average loss at step 22000: nan\n",
      "Average loss at step 24000: nan\n",
      "Average loss at step 26000: nan\n",
      "Average loss at step 28000: nan\n",
      "Average loss at step 30000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Average loss at step 32000: nan\n",
      "Average loss at step 34000: nan\n",
      "Average loss at step 36000: nan\n",
      "Average loss at step 38000: nan\n",
      "Average loss at step 40000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Average loss at step 42000: nan\n",
      "Average loss at step 44000: nan\n",
      "Average loss at step 46000: nan\n",
      "Average loss at step 48000: nan\n",
      "Average loss at step 50000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 52000: nan\n",
      "Average loss at step 54000: nan\n",
      "Average loss at step 56000: nan\n",
      "Average loss at step 58000: nan\n",
      "Average loss at step 60000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Average loss at step 62000: nan\n",
      "Average loss at step 64000: nan\n",
      "Average loss at step 66000: nan\n",
      "Average loss at step 68000: nan\n",
      "Average loss at step 70000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Average loss at step 72000: nan\n",
      "Average loss at step 74000: nan\n",
      "Average loss at step 76000: nan\n",
      "Average loss at step 78000: nan\n",
      "Average loss at step 80000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Average loss at step 82000: nan\n",
      "Average loss at step 84000: nan\n",
      "Average loss at step 86000: nan\n",
      "Average loss at step 88000: nan\n",
      "Average loss at step 90000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Average loss at step 92000: nan\n",
      "Average loss at step 94000: nan\n",
      "Average loss at step 96000: nan\n",
      "Average loss at step 98000: nan\n",
      "Average loss at step 100000: nan\n",
      "Nearest to ;: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to (: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to is: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to its: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to an: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to be: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to most: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to their: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to his: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to this: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to .: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to other: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to with: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to UNK: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to been: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n",
      "Nearest to or: non-indo-european, cherish, 15.1, 76.6, vetoes, ombudsman, matti, wood-based,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "skip_gram_loss_original = [] # Collect the sequential loss values for plotting purposes\n",
    "\n",
    "# ConfigProto is a way of providing various configuration settings \n",
    "# required to execute the graph\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    \n",
    "  # Initialize the variables in the graph\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "    \n",
    "  # Train the Word2vec model for num_step iterations\n",
    "  for step in range(num_steps):\n",
    "        \n",
    "    # Generate a single batch of data\n",
    "    batch_data, batch_labels = generate_batch_skip_gram(\n",
    "      batch_size, window_size)\n",
    "    \n",
    "    # Populate the feed_dict and run the optimizer (minimize loss)\n",
    "    # and compute the loss\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    # Update the average loss variable\n",
    "    average_loss += l\n",
    "\n",
    "    if (step+1) % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      skip_gram_loss_original.append(average_loss)\n",
    "      average_loss = 0\n",
    "    \n",
    "    # Here we compute the top_k closest words for a given validation word\n",
    "    # in terms of the cosine distance\n",
    "    # We do this for all the words in the validation set\n",
    "    # Note: This is an expensive step\n",
    "    if (step+1) % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "        \n",
    "  skip_gram_original_final_embeddings = normalized_embeddings.eval()\n",
    "np.save('skip_original_embeddings',skip_gram_original_final_embeddings)\n",
    "\n",
    "with open('skip_original_losses.csv', 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(skip_gram_loss_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Skip Gram Loss vs Original Skip Gram Loss\n",
    "Here we plot skip gram loss we got from chapter 3 with the original skip gram loss we just ran to see which one performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the skip-gram losses from the calculations we did in Chapter 3\n",
    "# So you need to make sure you have this csv file before running the code below\n",
    "skip_loss_path = os.path.join('..','ch3','skip_losses.csv')\n",
    "with open(skip_loss_path, 'rt') as f:\n",
    "    reader = csv.reader(f,delimiter=',')\n",
    "    for r_i,row in enumerate(reader):\n",
    "        if r_i == 0:\n",
    "            skip_gram_loss =  [float(s) for s in row]\n",
    "\n",
    "\n",
    "pylab.figure(figsize=(15,5))  # figure in inches\n",
    "\n",
    "# Define the x axis\n",
    "x = np.arange(len(skip_gram_loss))*2000\n",
    "  \n",
    "# Plot the skip_gram_loss (loaded from chapter 3)\n",
    "pylab.plot(x, skip_gram_loss, label=\"Skip-Gram (Improved)\",linestyle='--',linewidth=2)    \n",
    "# Plot the original skip gram loss from what we just ran\n",
    "pylab.plot(x, skip_gram_loss_original, label=\"Skip-Gram (Original)\",linewidth=2)\n",
    "\n",
    "# Set some text around the plot\n",
    "pylab.title('Original vs Improved Skip-Gram Loss Decrease Over Time',fontsize=24)\n",
    "pylab.xlabel('Iterations',fontsize=22)\n",
    "pylab.ylabel('Loss',fontsize=22)\n",
    "pylab.legend(loc=1,fontsize=22)\n",
    "\n",
    "# use for saving the figure if needed\n",
    "pylab.savefig('loss_skipgram_original_vs_impr.png')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Skip-Gram Loss vs CBOW Loss\n",
    "\n",
    "Here we compare the skip-gram loss and CBOW loss to compare which loss decreases quicker. Refer the text for an analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\ch3\\\\cbow_losses.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c1bbfc1bade9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# So you need to make sure you have this csv file before running the code below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcbow_loss_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ch3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cbow_losses.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbow_loss_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mr_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\ch3\\\\cbow_losses.csv'"
     ]
    }
   ],
   "source": [
    "# Load the skip-gram losses from the calculations we did in Chapter 3\n",
    "# So you need to make sure you have this csv file before running the code below\n",
    "cbow_loss_path = os.path.join('..','ch3','cbow_losses.csv')\n",
    "with open(cbow_loss_path, 'rt') as f:\n",
    "    reader = csv.reader(f,delimiter=',')\n",
    "    for r_i,row in enumerate(reader):\n",
    "        if r_i == 0:\n",
    "            cbow_loss =  [float(s) for s in row]\n",
    "\n",
    "pylab.figure(figsize=(15,5))  # in inches\n",
    "\n",
    "# Define the x axis\n",
    "x = np.arange(len(skip_gram_loss))*2000\n",
    "\n",
    "# Plot the skip_gram_loss (loaded from chapter 3)\n",
    "pylab.plot(x, skip_gram_loss, label=\"Skip-Gram\",linestyle='--',linewidth=2)    \n",
    "# Plot the cbow_loss (loaded from chapter 3)\n",
    "pylab.plot(x, cbow_loss, label=\"CBOW\",linewidth=2)\n",
    "\n",
    "# Set some text around the plot\n",
    "pylab.title('Skip-Gram vs CBOW Loss Decrease Over Time',fontsize=24)\n",
    "pylab.xlabel('Iterations',fontsize=22)\n",
    "pylab.ylabel('Loss',fontsize=22)\n",
    "pylab.legend(loc=1,fontsize=22)\n",
    "\n",
    "# use for saving the figure if needed\n",
    "pylab.savefig('loss_skipgram_vs_cbow.png')\n",
    "pylab.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting TSNE Embeddings for Skip-Gram and CBOW Side by Side\n",
    "\n",
    "Loss itself is not an adequate measure of performance. Therefore we visualize the learned embeddings by projecting the embeddings to a two dimensional canvas with a technique known as t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clustered_embeddings(embeddings,distance_threshold,sample_threshold):\n",
    "    ''' \n",
    "    Find only the closely clustered embeddings. \n",
    "    This gets rid of more sparsly distributed word embeddings and make the visualization clearer\n",
    "    This is useful for t-SNE visualization\n",
    "    \n",
    "    distance_threshold: maximum distance between two points to qualify as neighbors\n",
    "    sample_threshold: number of neighbors required to be considered a cluster\n",
    "    '''\n",
    "    \n",
    "    # calculate cosine similarity\n",
    "    cosine_sim = np.dot(embeddings,np.transpose(embeddings))\n",
    "    norm = np.dot(np.sum(embeddings**2,axis=1).reshape(-1,1),np.sum(np.transpose(embeddings)**2,axis=0).reshape(1,-1))\n",
    "    assert cosine_sim.shape == norm.shape\n",
    "    cosine_sim /= norm\n",
    "    \n",
    "    # make all the diagonal entries zero otherwise this will be picked as highest\n",
    "    np.fill_diagonal(cosine_sim, -1.0)\n",
    "    \n",
    "    argmax_cos_sim = np.argmax(cosine_sim, axis=1)\n",
    "    mod_cos_sim = cosine_sim\n",
    "    # find the maximums in a loop to count if there are more than n items above threshold\n",
    "    for _ in range(sample_threshold-1):\n",
    "        argmax_cos_sim = np.argmax(cosine_sim, axis=1)\n",
    "        mod_cos_sim[np.arange(mod_cos_sim.shape[0]),argmax_cos_sim] = -1\n",
    "    \n",
    "    max_cosine_sim = np.max(mod_cos_sim,axis=1)\n",
    "\n",
    "    return np.where(max_cosine_sim>distance_threshold)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the skip-gram and CBOW embeddings to a t-SNE \n",
    "We fit skip-gram and CBOW embeddings to a t-SNE to get their mapping on a two dimensional surface. We only visualize densely clustered data points to avoid clutter in the visualization. This is achieved with the above function `find_clustered_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\ch3\\\\cbow_embeddings.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-72f820e142ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mskip_gram_final_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskip_emb_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcbow_final_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbow_emb_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnum_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;31m# we will use a large sample space to build the T-SNE manifold and then prune it using cosine similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tensorflow1.15-cpu\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\ch3\\\\cbow_embeddings.npy'"
     ]
    }
   ],
   "source": [
    "# Load the previously saved embeddings from Chapter 3 exercise\n",
    "skip_emb_path = os.path.join('..','ch3','skip_embeddings.npy')\n",
    "cbow_emb_path = os.path.join('..','ch3','cbow_embeddings.npy')\n",
    "\n",
    "skip_gram_final_embeddings = np.load(skip_emb_path)\n",
    "cbow_final_embeddings = np.load(cbow_emb_path)\n",
    "\n",
    "num_points = 1000 # we will use a large sample space to build the T-SNE manifold and then prune it using cosine similarity\n",
    "\n",
    "# Create a t-SNE object from scikit-learn\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "print('Fitting embeddings to T-SNE (skip-gram and CBOW)')\n",
    "# Get the T-SNE manifold for skip-gram embeddings\n",
    "print('\\tSkip-gram')\n",
    "sg_selected_embeddings = skip_gram_final_embeddings[:num_points, :]\n",
    "sg_two_d_embeddings = tsne.fit_transform(sg_selected_embeddings)\n",
    "\n",
    "# Get the T-SNE manifold for CBOW embeddings\n",
    "print('\\tCBOW')\n",
    "cbow_selected_embeddings = cbow_final_embeddings[:num_points, :]\n",
    "cbow_two_d_embeddings = tsne.fit_transform(cbow_selected_embeddings)\n",
    "\n",
    "print('Pruning the T-SNE embeddings (skip-gram and CBOW)')\n",
    "# Prune the embeddings by getting ones only more than n-many sample above the similarity threshold\n",
    "# this unclutters the visualization\n",
    "# Prune skip-gram\n",
    "print('\\tSkip-gram')\n",
    "sg_selected_ids = find_clustered_embeddings(sg_selected_embeddings,.3,10)\n",
    "sg_two_d_embeddings = sg_two_d_embeddings[sg_selected_ids,:]\n",
    "# Prune CBOW\n",
    "print('\\tCBOW')\n",
    "cbow_selected_ids = find_clustered_embeddings(cbow_selected_embeddings,.3,10)\n",
    "cbow_two_d_embeddings = cbow_two_d_embeddings[cbow_selected_ids,:]\n",
    "\n",
    "# Some stats about pruning\n",
    "print('Out of ',num_points,' samples (skip-gram), ', sg_selected_ids.shape[0],' samples were selected by pruning')\n",
    "print('Out of ',num_points,' samples (CBOW), ', cbow_selected_ids.shape[0],' samples were selected by pruning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Embeddings\n",
    "Here we plot the embeddings side by side, each embedding layer on its own subplot. We also use different colors for data points to improve clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sg_selected_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c012e0fdbb6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;31m# Run the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m \u001b[0msg_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msg_selected_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[0mcbow_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcbow_selected_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[0mplot_embeddings_side_by_side\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msg_two_d_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_two_d_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcbow_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sg_selected_ids' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_embeddings_side_by_side(sg_embeddings, cbow_embeddings, sg_labels, cbow_labels):\n",
    "  ''' Plots word embeddings of skip-gram and CBOW side by side as subplots\n",
    "  '''\n",
    "  # number of clusters for each word embedding \n",
    "  # clustering is used to assign different colors as a visual aid\n",
    "  n_clusters = 20 \n",
    "    \n",
    "  # automatically build a discrete set of colors, each for cluster\n",
    "  print('Define Label colors for %d',n_clusters)\n",
    "  label_colors = [pylab.cm.spectral(float(i) /n_clusters) for i in range(n_clusters)]\n",
    "  \n",
    "  # Make sure number of embeddings and their labels are the same\n",
    "  assert sg_embeddings.shape[0] >= len(sg_labels), 'More labels than embeddings'\n",
    "  assert cbow_embeddings.shape[0] >= len(cbow_labels), 'More labels than embeddings'\n",
    "  \n",
    "  print('Running K-Means for skip-gram')\n",
    "  # Define K-Means\n",
    "  sg_kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=0).fit(sg_embeddings)\n",
    "  sg_kmeans_labels = sg_kmeans.labels_\n",
    "  sg_cluster_centroids = sg_kmeans.cluster_centers_\n",
    "\n",
    "  print('Running K-Means for CBOW')\n",
    "  cbow_kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=0).fit(cbow_embeddings)\n",
    "  cbow_kmeans_labels = cbow_kmeans.labels_\n",
    "  cbow_cluster_centroids = cbow_kmeans.cluster_centers_\n",
    "    \n",
    "  print('K-Means ran successfully')\n",
    "\n",
    "  print('Plotting results')\n",
    "  pylab.figure(figsize=(25,20))  # in inches\n",
    "\n",
    "  # Get the first subplot\n",
    "  pylab.subplot(1, 2, 1) \n",
    "\n",
    "  # Plot all the embeddings and their corresponding words for skip-gram\n",
    "  for i, (label,klabel) in enumerate(zip(sg_labels,sg_kmeans_labels)):\n",
    "    center = sg_cluster_centroids[klabel,:]\n",
    "    x, y = cbow_embeddings[i,:]\n",
    "    \n",
    "    # This is just to spread the data points around a bit\n",
    "    # So that the labels are clearer\n",
    "    # We repel datapoints from the cluster centroid\n",
    "    if x < center[0]:\n",
    "        x += -abs(np.random.normal(scale=2.0))\n",
    "    else:\n",
    "        x += abs(np.random.normal(scale=2.0))\n",
    "        \n",
    "    if y < center[1]:\n",
    "        y += -abs(np.random.normal(scale=2.0))\n",
    "    else:\n",
    "        y += abs(np.random.normal(scale=2.0))\n",
    "        \n",
    "    pylab.scatter(x, y, c=label_colors[klabel])    \n",
    "    x = x if np.random.random()<0.5 else x + 10\n",
    "    y = y if np.random.random()<0.5 else y - 10\n",
    "    pylab.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points',\n",
    "                   ha='right', va='bottom',fontsize=16)\n",
    "  pylab.title('t-SNE for Skip-Gram',fontsize=24)\n",
    "\n",
    "  # Get the second subplot\n",
    "  pylab.subplot(1, 2, 2)  \n",
    "\n",
    "  # Plot all the embeddings and their corresponding words for CBOW\n",
    "  for i, (label,klabel) in enumerate(zip(cbow_labels,cbow_kmeans_labels)):\n",
    "    center = cbow_cluster_centroids[klabel,:]\n",
    "    x, y = cbow_embeddings[i,:]  \n",
    "    \n",
    "    # This is just to spread the data points around a bit\n",
    "    # So that the labels are clearer\n",
    "    # We repel datapoints from the cluster centroid\n",
    "    if x < center[0]:\n",
    "        x += -abs(np.random.normal(scale=2.0))\n",
    "    else:\n",
    "        x += abs(np.random.normal(scale=2.0))\n",
    "        \n",
    "    if y < center[1]:\n",
    "        y += -abs(np.random.normal(scale=2.0))\n",
    "    else:\n",
    "        y += abs(np.random.normal(scale=2.0))\n",
    "        \n",
    "    pylab.scatter(x, y, c=label_colors[klabel])  \n",
    "    x = x if np.random.random()<0.5 else x + np.random.randint(0,10)\n",
    "    y = y + np.random.randint(0,5) if np.random.random()<0.5 else y - np.random.randint(0,5)\n",
    "    pylab.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points',\n",
    "                   ha='right', va='bottom',fontsize=16)\n",
    "\n",
    "  pylab.title('t-SNE for CBOW',fontsize=24)\n",
    "  # use for saving the figure if needed\n",
    "  pylab.savefig('tsne_skip_vs_cbow.png')\n",
    "  pylab.show()\n",
    "\n",
    "# Run the function\n",
    "sg_words = [reverse_dictionary[i] for i in sg_selected_ids]\n",
    "cbow_words = [reverse_dictionary[i] for i in cbow_selected_ids]\n",
    "plot_embeddings_side_by_side(sg_two_d_embeddings, cbow_two_d_embeddings, sg_words,cbow_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the data generation process\n",
    "We need to define a new data generator for CBOW. Shape of the new input array is (batch_size, context_window*2). That is, a batch in CBOW captures all the words in the context of a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with window_size = 1:\n",
      "    batch: [['propaganda', 'a'], ['is', 'concerted'], ['a', 'set'], ['concerted', 'of'], ['set', 'messages'], ['of', 'aimed'], ['messages', 'at'], ['aimed', 'influencing']]\n",
      "    labels: ['is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: [['propaganda', 'is', 'concerted', 'set'], ['is', 'a', 'set', 'of'], ['a', 'concerted', 'of', 'messages'], ['concerted', 'set', 'messages', 'aimed'], ['set', 'of', 'aimed', 'at'], ['of', 'messages', 'at', 'influencing'], ['messages', 'aimed', 'influencing', 'the'], ['aimed', 'at', 'the', 'opinions']]\n",
      "    labels: ['a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch_cbow(batch_size, window_size):\n",
    "    # window_size is the amount of words we're looking at from each side of a given word\n",
    "    # creates a single batch\n",
    "    \n",
    "    # data_index is updated by 1 everytime we read a set of data point\n",
    "    global data_index\n",
    "\n",
    "    # span defines the total window size, where\n",
    "    # data we consider at an instance looks as follows. \n",
    "    # [ skip_window target skip_window ]\n",
    "    # e.g if skip_window = 2 then span = 5\n",
    "    span = 2 * window_size + 1 # [ skip_window target skip_window ]\n",
    "\n",
    "    # two numpy arras to hold target words (batch)\n",
    "    # and context words (labels)\n",
    "    # Note that batch has span-1=2*window_size columns\n",
    "    batch = np.ndarray(shape=(batch_size,span-1), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # The buffer holds the data contained within the span\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "\n",
    "    # Fill the buffer and update the data_index\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # Here we do the batch reading\n",
    "    # We iterate through each batch index\n",
    "    # For each batch index, we iterate through span elements\n",
    "    # to fill in the columns of batch array\n",
    "    for i in range(batch_size):\n",
    "        target = window_size  # target label at the center of the buffer\n",
    "        target_to_avoid = [ window_size ] # we only need to know the words around a given word, not the word itself\n",
    "\n",
    "        # add selected target to avoid_list for next time\n",
    "        col_idx = 0\n",
    "        for j in range(span):\n",
    "            # ignore the target word when creating the batch\n",
    "            if j==span//2:\n",
    "                continue\n",
    "            batch[i,col_idx] = buffer[j] \n",
    "            col_idx += 1\n",
    "        labels[i, 0] = buffer[target]\n",
    "\n",
    "        # Everytime we read a data point,\n",
    "        # we need to move the span by 1\n",
    "        # to create a fresh new span\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    return batch, labels\n",
    "\n",
    "for window_size in [1,2]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch_cbow(batch_size=8, window_size=window_size)\n",
    "    print('\\nwith window_size = %d:' % (window_size))\n",
    "    print('    batch:', [[reverse_dictionary[bii] for bii in bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using Candidate Sampling with the Unigram Distribution for Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram distribution\n",
    "The unigram distribution $U(w_i)$ for a given word $w_i$ is given by,\n",
    "\n",
    "$ P(w_i|w_1,...,w_{i-1}) \\simeq P(w_i) = \\frac{count(w_i)}{\\sum_{w_j \\in corpus}count(w_j)} = U(w_i) $\n",
    "\n",
    "The original paper found that if words are sampled the noise for the negative sampling with a particular distribution gives the best results. And the distribution is given by,\n",
    "\n",
    "$ U(w)^{3/4} / Z $ where $Z$ is a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Unigram probabilities\n",
      "[0.02045303287830911, 0.06750330907324231, 0.05474600984823039, 0.03597481028426345, 0.03460744677591096, 0.026371134468419585, 0.023147595823293555, 0.019527771670524895, 0.017548129202166005, 0.009180614260387545]\n"
     ]
    }
   ],
   "source": [
    "# Creating vocabulary file and unigram counts\n",
    "# Requried by the tf.nn.fixed_unigram_candidate_sampler\n",
    "\n",
    "# vocabulary file: Each valid line in this file (which should have a CSV-like format) corresponds to a valid word ID. IDs are in sequential order.\n",
    "# unigrams: A list of unigram counts or probabilities, one per ID in sequential order. Exactly one of vocab_file and unigrams should be passed to this operation.\n",
    "\n",
    "word_count_dictionary = {}\n",
    "unigrams = [0 for _ in range(vocabulary_size)]\n",
    "for word,w_count in count:\n",
    "    w_idx = dictionary[word]\n",
    "    unigrams[w_idx] = w_count*1.0/token_count\n",
    "    word_count_dictionary[w_idx] = w_count\n",
    "print('First 10 Unigram probabilities')\n",
    "print(unigrams[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters\n",
    "\n",
    "Here we define several hyperparameters including `batch_size` (amount of samples in a single batch) `embedding_size` (size of embedding vectors) `window_size` (context window size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # Data points in a single batch\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "# How many words to consider left and right.\n",
    "# Skip gram by design does not require to have all the context words in a given step\n",
    "# However, for CBOW that's a requirement, so we limit the window size\n",
    "window_size = 2 \n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 50\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
    "\n",
    "num_sampled = 32 # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Inputs and Outputs\n",
    "\n",
    "Here we define placeholders for feeding in training inputs and outputs (each of size `batch_size`) and a constant tensor to contain validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training input data (target word IDs). Note that it has 2*window_size columns\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size,2*window_size])\n",
    "# Training input label data (context word IDs)\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "# Validation input data, we don't need a placeholder\n",
    "# as we have already defined the IDs of the words selected\n",
    "# as validation data\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model Parameters and Other Variables\n",
    "We now define several TensorFlow variables such as an embedding layer (`embeddings`) and neural network parameters (`softmax_weights` and `softmax_biases`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables.\n",
    "\n",
    "# Embedding layer, contains the word embeddings\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0,dtype=tf.float32))\n",
    "\n",
    "# Softmax Weights and Biases\n",
    "softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                 stddev=0.5 / math.sqrt(embedding_size),dtype=tf.float32))\n",
    "softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Computations\n",
    "\n",
    "We first defing a lookup function to fetch the corresponding embedding vectors for a set of given inputs. Concretely, we define $2\\times$`window_size` embedding lookups. We then concatenate all these looked up embedding vectors to form a matrix of size `[batch_size, embedding_size, 2*window_size]`. Thereafter, we average these embedding lookups to produce an average embeddings of size `[batch_size, embedding_size]`. With that, we define negative sampling loss function `tf.nn.sampled_softmax_loss` which takes in the embedding vectors and previously defined neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining 4 embedding lookups representing each word in the context\n",
      "Stacked embedding size: [128, 128, 4]\n",
      "Reduced mean embedding size: [128, 128]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model.\n",
    "# Look up embeddings for a batch of inputs.\n",
    "# Here we do embedding lookups for each column in the input placeholder\n",
    "# and then average them to produce an embedding_size word vector\n",
    "stacked_embedings = None\n",
    "print('Defining %d embedding lookups representing each word in the context'%(2*window_size))\n",
    "for i in range(2*window_size):\n",
    "    embedding_i = tf.nn.embedding_lookup(embeddings, train_dataset[:,i])        \n",
    "    x_size,y_size = embedding_i.get_shape().as_list()\n",
    "    if stacked_embedings is None:\n",
    "        stacked_embedings = tf.reshape(embedding_i,[x_size,y_size,1])\n",
    "    else:\n",
    "        stacked_embedings = tf.concat(axis=2,values=[stacked_embedings,tf.reshape(embedding_i,[x_size,y_size,1])])\n",
    "\n",
    "assert stacked_embedings.get_shape().as_list()[2]==2*window_size\n",
    "print(\"Stacked embedding size: %s\"%stacked_embedings.get_shape().as_list())\n",
    "mean_embeddings =  tf.reduce_mean(stacked_embedings,2,keepdims=False)\n",
    "print(\"Reduced mean embedding size: %s\"%mean_embeddings.get_shape().as_list())\n",
    "\n",
    "# Compute the softmax loss, using a sample of the negative labels each time.\n",
    "# inputs are embeddings of the train words\n",
    "# with this loss we optimize weights, biases, embeddings\n",
    "\n",
    "# However, unlike at the previous instance (Chapter 3) we use a different sample to sampel negative classes\n",
    "# Particularly we use a unigram candidate sampler, to which we provide\n",
    "# the unigram probabilities we computed earlier. For details about the passed arguments\n",
    "# Refer the text in Chapter 4\n",
    "candidate_sampler = tf.nn.fixed_unigram_candidate_sampler(true_classes = tf.cast(train_labels,dtype=tf.int64), num_true = 1, \n",
    "                                      num_sampled = num_sampled, \n",
    "                                      unique = True, range_max = vocabulary_size, \n",
    "                                      distortion=0.75, \n",
    "                                      num_reserved_ids=0, \n",
    "                                      unigrams=unigrams, name='unigram_sampler')\n",
    "\n",
    "# The loss is very similar to what we defined in Chapter 3, except for\n",
    "# passing the above defined sampler to the function.\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=mean_embeddings,\n",
    "                           labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size,\n",
    "                              sampled_values=candidate_sampler))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter Optimizer\n",
    "\n",
    "We then define a learning rate as a constant and an optimizer which uses the Adagrad method. Feel free to experiment with other optimizers listed [here](https://www.tensorflow.org/api_guides/python/train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Word Similarities \n",
    "We calculate the similarity between two given words in terms of the cosine distance. To do this efficiently we use matrix operations to do so, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity between minibatch examples and all embeddings.\n",
    "# We use the cosine distance:\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running CBOW with Unigram Candidate Sampling\n",
    "\n",
    "Here we run the CBOW algorithm with unigram based candidate sampling, we defined above. Specifically, we first initialize variables, and then train the algorithm for many steps (`num_steps`). And every few steps we evaluate the algorithm on a fixed validation set and print out the words that appear to be closest for a given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 2000: 3.434522\n",
      "Average loss at step 4000: 2.908296\n",
      "Average loss at step 6000: 2.776565\n",
      "Average loss at step 8000: 2.690833\n",
      "Average loss at step 10000: 2.634494\n",
      "Nearest to was: is, were, obscura, had, be, pondi, shiite, has,\n",
      "Nearest to 's: lizard-like, two-third, , dictate, beartooth, tastefully, his, prolific,\n",
      "Nearest to ``: duality, purify, patrician, chlorophyll, bonds, spilled, annual, supports,\n",
      "Nearest to (: wanda, =, liberalised, cavalrymen, lawless, ovations, 10, suppose,\n",
      "Nearest to their: its, his, the, employee, hutu-dominated, unmatched, simplest, plantation,\n",
      "Nearest to is: was, are, has, surnames, were, state-controlled, fins, offences,\n",
      "Nearest to which: but, where, self-given, that, confession, sightings, opining, prostate,\n",
      "Nearest to UNK: references-small, spite, clone, pontificio, solidified, papuan, reviewer, anti-habsburg,\n",
      "Nearest to been: sororities, synaptic, gmez, become, snoop, environment, sheriffdoms, abbey,\n",
      "Nearest to the: its, a, another, an, any, this, their, these,\n",
      "Nearest to not: conceptualization, 1868, hua, latinos, newry, trim, castilians, asante,\n",
      "Nearest to are: were, is, osmond, lpez, 1845, pox, exaggerated, have,\n",
      "Nearest to has: have, had, is, jean-jacques, gnosis, year, antiplatelet, two-time,\n",
      "Nearest to ,: ;, ., contractors, juhu, universalism, buttes, run-off, refrigerate,\n",
      "Nearest to be: have, raka, pietermaritzburg, christo, was, meghna, manufacturing, holodomor,\n",
      "Nearest to one: 2,042, jazbo, overall, nosocomial, safely, hadley, kuchma, aptian,\n",
      "Average loss at step 12000: 2.607142\n",
      "Average loss at step 14000: 2.549052\n",
      "Average loss at step 16000: 2.513077\n",
      "Average loss at step 18000: 2.497698\n",
      "Average loss at step 20000: 2.475539\n",
      "Nearest to was: is, were, had, be, became, obscura, has, celebrates,\n",
      "Nearest to 's: lizard-like, empowerment, his, prolific, , dictate, curtailed, multiverse,\n",
      "Nearest to ``: duality, patrician, supports, purify, discovering, spinosaurus, dust, 1483,\n",
      "Nearest to (: liberalised, =, lawless, wanda, hypothetically, copying, lower-income, cavalrymen,\n",
      "Nearest to their: its, his, her, hutu-dominated, our, employee, plantation, christianity,\n",
      "Nearest to is: was, are, has, surnames, condemning, obscura, bell-like, shipwreck,\n",
      "Nearest to which: but, where, who, that, what, self-given, cognitive, opining,\n",
      "Nearest to UNK: starter, ganondorf, 4, skip, 2, solidified, halloween, phases,\n",
      "Nearest to been: become, be, gmez, sheriffdoms, sororities, moving, shipments, pigeon,\n",
      "Nearest to the: its, another, a, any, fota, smelt, al-aqsa, cobblers,\n",
      "Nearest to not: conceptualization, 1868, latinos, newry, hinterlands, carney, performed, trim,\n",
      "Nearest to are: were, is, have, osmond, lpez, exaggerated, meghna, mihly,\n",
      "Nearest to has: had, have, is, antiplatelet, e.d, was, lbeck, adriaen,\n",
      "Nearest to ,: ;, ., contractors, juhu, buttes, mcluhan, gurzuf, refrigerate,\n",
      "Nearest to be: being, have, raka, been, christo, was, were, flames,\n",
      "Nearest to one: 2,042, 1623, hadley, overall, jazbo, kuchma, beggars, trevor,\n",
      "Average loss at step 22000: 2.444296\n",
      "Average loss at step 24000: 2.430544\n",
      "Average loss at step 26000: 2.407561\n",
      "Average loss at step 28000: 2.335586\n",
      "Average loss at step 30000: 2.265266\n",
      "Nearest to was: is, were, became, be, obscura, had, categories, 737,\n",
      "Nearest to 's: empowerment, his, curtailed, prolific, her, lizard-like, tastefully, ,\n",
      "Nearest to ``: duality, patrician, supports, discovering, spinosaurus, spilled, purify, 64.8,\n",
      "Nearest to (: liberalised, wanda, lawless, =, hypothetically, prospected, ovations, copying,\n",
      "Nearest to their: its, his, her, our, plantation, employee, quips, christianity,\n",
      "Nearest to is: was, are, has, surnames, obscura, became, cue, versus,\n",
      "Nearest to which: but, where, what, opining, who, that, whose, cognitive,\n",
      "Nearest to UNK: saurischia, ostrom, ganondorf, hon, skip, clone, loggia, speaker-elect,\n",
      "Nearest to been: become, be, gmez, sheriffdoms, sororities, were, shipments, pigeon,\n",
      "Nearest to the: its, another, wichita, dat, fota, every, zt, al-aqsa,\n",
      "Nearest to not: conceptualization, 1868, n't, newry, latinos, chrooting, never, 1271,\n",
      "Nearest to are: were, is, have, osmond, exaggerated, mihly, meghna, lpez,\n",
      "Nearest to has: have, had, is, having, adriaen, e.d, antiplatelet, lbeck,\n",
      "Nearest to ,: ;, spent, ., juhu, conquering, contractors, prepared, configuration,\n",
      "Nearest to be: being, christo, been, raka, was, meghna, have, were,\n",
      "Nearest to one: beggars, 2,042, hadley, 1623, schumann, trevor, decades-long, overall,\n",
      "Average loss at step 32000: 2.273416\n",
      "Average loss at step 34000: 2.260519\n",
      "Average loss at step 36000: 2.251657\n",
      "Average loss at step 38000: 2.218568\n",
      "Average loss at step 40000: 2.222517\n",
      "Nearest to was: is, were, became, obscura, be, had, ibac, has,\n",
      "Nearest to 's: , prolific, haruna, his, empowerment, dictate, , lizard-like,\n",
      "Nearest to ``: discovering, duality, purify, patrician, supports, !, storyline, spinosaurus,\n",
      "Nearest to (: =, liberalised, lawless, wanda, lower-income, jurists, prospected, copying,\n",
      "Nearest to their: its, his, her, our, employee, civilians, plantation, christianity,\n",
      "Nearest to is: was, are, has, cue, obscura, surnames, became, gebhart,\n",
      "Nearest to which: where, but, what, that, who, cognitive, opining, whose,\n",
      "Nearest to UNK: skip, saurischia, loggia, ostrom, voluntary, ganondorf, pasture, starter,\n",
      "Nearest to been: become, be, gmez, sheriffdoms, were, sororities, versus, pigeon,\n",
      "Nearest to the: another, its, a, any, atolls, 256, every, cathar,\n",
      "Nearest to not: n't, conceptualization, 1868, latinos, newry, chrooting, never, 1271,\n",
      "Nearest to are: were, is, have, mihly, osmond, marable, exaggerated, lpez,\n",
      "Nearest to has: had, have, is, having, antiplatelet, e.d, was, lbeck,\n",
      "Nearest to ,: ;, ., , gurzuf, refrigerate, -, juhu, contractors,\n",
      "Nearest to be: being, been, christo, raka, was, have, flames, were,\n",
      "Nearest to one: beggars, 2,042, 1623, khayelitsha, kuchma, hadley, trevor, alarmed,\n",
      "Average loss at step 42000: 2.193669\n",
      "Average loss at step 44000: 2.206131\n",
      "Average loss at step 46000: 2.190801\n",
      "Average loss at step 48000: 2.179392\n",
      "Average loss at step 50000: 2.204391\n",
      "Nearest to was: is, were, became, be, being, ibac, been, pondi,\n",
      "Nearest to 's: empowerment, s, , prolific, curtailed, haruna, , his,\n",
      "Nearest to ``: patrician, discovering, duality, supports, tai-ngon, , 1483, dust,\n",
      "Nearest to (: =, liberalised, wanda, lawless, lower-income, copying, hypothetically, prospected,\n",
      "Nearest to their: its, his, her, our, employee, civilians, the, plantation,\n",
      "Nearest to is: was, are, has, remains, became, allows, obscura, surnames,\n",
      "Nearest to which: where, but, what, that, whose, opining, who, flavorings,\n",
      "Nearest to UNK: ganondorf, skip, saurischia, 1500, ostrom, pontificio, protists, loggia,\n",
      "Nearest to been: become, be, gmez, sheriffdoms, were, was, versus, 55,000,\n",
      "Nearest to the: its, a, another, their, tai, artifact, al-aqsa, any,\n",
      "Nearest to not: n't, conceptualization, 1868, newry, latinos, never, chrooting, carney,\n",
      "Nearest to are: were, is, have, mihly, marable, osmond, lpez, be,\n",
      "Nearest to has: have, had, having, is, e.d, antiplatelet, tln, lbeck,\n",
      "Nearest to ,: ;, ., , conquering, contractors, relinquish, spent, mortimer,\n",
      "Nearest to be: being, been, christo, was, raka, were, are, meghna,\n",
      "Nearest to one: beggars, 2,042, hadley, khayelitsha, yahweh, trevor, shaoqi, a,\n",
      "Average loss at step 52000: 2.147333\n",
      "Average loss at step 54000: 2.110350\n",
      "Average loss at step 56000: 2.084034\n",
      "Average loss at step 58000: 2.072973\n",
      "Average loss at step 60000: 2.063141\n",
      "Nearest to was: is, were, became, ibac, had, obscura, 737, be,\n",
      "Nearest to 's: , prolific, s, empowerment, his, , expanded, izaak,\n",
      "Nearest to ``: discovering, , duality, patrician, storyline, tai-ngon, purify, !,\n",
      "Nearest to (: liberalised, lawless, =, wanda, lower-income, prospected, hypothetically, copying,\n",
      "Nearest to their: its, his, her, our, civilians, employee, quips, plantation,\n",
      "Nearest to is: was, are, has, remains, cue, obscura, allows, stormberg,\n",
      "Nearest to which: where, but, what, whose, opining, cognitive, flavorings, who,\n",
      "Nearest to UNK: la, pasture, saurischia, skip, ostrom, carnap, locus, mississippi,\n",
      "Nearest to been: become, be, gmez, sheriffdoms, versus, 55,000, debris, sororities,\n",
      "Nearest to the: its, dat, their, fremantle, anti-semitic, another, al-aqsa, quan,\n",
      "Nearest to not: n't, never, conceptualization, 1868, newry, latinos, chrooting, encyclopedias,\n",
      "Nearest to are: were, is, mihly, osmond, have, include, marable, exaggerated,\n",
      "Nearest to has: have, had, having, is, e.d, antiplatelet, endlessly, postmodernists,\n",
      "Nearest to ,: ;, , ., contractors, gurzuf, saint-pierre, juhu, characterisation,\n",
      "Nearest to be: being, been, christo, raka, lead, were, was, are,\n",
      "Nearest to one: beggars, yahweh, 2,042, khayelitsha, hadley, shaoqi, irinn, thrust,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 62000: 2.085484\n",
      "Average loss at step 64000: 2.036684\n",
      "Average loss at step 66000: 2.058729\n",
      "Average loss at step 68000: 2.023039\n",
      "Average loss at step 70000: 2.032870\n",
      "Nearest to was: is, were, became, ibac, obscura, had, been, 737,\n",
      "Nearest to 's: , s, prolific, empowerment, my, , blooming, curtailed,\n",
      "Nearest to ``: discovering, patrician, duality, , ?, diracodon, !, tai-ngon,\n",
      "Nearest to (: liberalised, =, copying, lawless, hypothetically, lower-income, corms, wanda,\n",
      "Nearest to their: its, his, her, our, civilians, employee, quips, commodores,\n",
      "Nearest to is: was, are, remains, has, allows, contains, obscura, cue,\n",
      "Nearest to which: where, but, what, whose, opining, cognitive, that, starving,\n",
      "Nearest to UNK: microprogram, la, ganondorf, voluntary, skip, halloween, saurischia, ostrom,\n",
      "Nearest to been: become, be, gmez, were, 55,000, debris, sororities, was,\n",
      "Nearest to the: a, dat, auroral, artifact, columnist, fenced, zt, another,\n",
      "Nearest to not: n't, never, conceptualization, latinos, newry, 1868, chrooting, now,\n",
      "Nearest to are: were, is, mihly, have, osmond, marable, four-month, include,\n",
      "Nearest to has: have, had, having, is, e.d, lbeck, antiplatelet, includes,\n",
      "Nearest to ,: , ;, gurzuf, ., -, conquering, spent, juhu,\n",
      "Nearest to be: being, been, christo, raka, lead, argumentation, stigmasarranged, were,\n",
      "Nearest to one: beggars, yahweh, hadley, khayelitsha, 2,042, shaoqi, trevor, thrust,\n",
      "Average loss at step 72000: 2.028619\n",
      "Average loss at step 74000: 2.006661\n",
      "Average loss at step 76000: 2.066641\n",
      "Average loss at step 78000: 2.001895\n",
      "Average loss at step 80000: 1.950626\n",
      "Nearest to was: is, were, became, ibac, been, 737, obscura, pondi,\n",
      "Nearest to 's: , s, empowerment, my, prolific, curtailed, haruna, violins,\n",
      "Nearest to ``: , discovering, patrician, duality, ?, storyline, diracodon, degrade,\n",
      "Nearest to (: liberalised, lower-income, copying, lawless, wanda, prospected, =, anticipate,\n",
      "Nearest to their: its, his, her, our, civilians, employee, quips, plantation,\n",
      "Nearest to is: was, are, remains, has, vellum, contains, obscura, cue,\n",
      "Nearest to which: where, but, what, opining, whose, cognitive, airplanes, flavorings,\n",
      "Nearest to UNK: saurischia, la, ganondorf, ostrom, memorized, microprogram, skip, c.,\n",
      "Nearest to been: become, be, gmez, was, 55,000, sheriffdoms, were, cranwell,\n",
      "Nearest to the: fenced, fota, al-aqsa, zt, d'italia, a, another, every,\n",
      "Nearest to not: n't, never, conceptualization, 1868, newry, chrooting, latinos, precedence,\n",
      "Nearest to are: were, is, mihly, osmond, marable, have, include, meghna,\n",
      "Nearest to has: have, had, having, is, hosts, e.d, includes, lbeck,\n",
      "Nearest to ,: ;, , contractors, -, conquering, times-herald, spent, .,\n",
      "Nearest to be: being, been, christo, raka, become, lead, was, girdle,\n",
      "Nearest to one: beggars, yahweh, hadley, 2,042, mckinley, thrust, tempting, schumann,\n",
      "Average loss at step 82000: 1.963765\n",
      "Average loss at step 84000: 1.944442\n",
      "Average loss at step 86000: 1.945483\n",
      "Average loss at step 88000: 1.930794\n",
      "Average loss at step 90000: 1.930307\n",
      "Nearest to was: is, were, became, ibac, obscura, had, 737, beatrice,\n",
      "Nearest to 's: , s, prolific, empowerment, haruna, , my, deccan,\n",
      "Nearest to ``: , discovering, patrician, duality, purify, !, storyline, degrade,\n",
      "Nearest to (: liberalised, lawless, lower-income, copying, prospected, wanda, jurists, anticipate,\n",
      "Nearest to their: its, his, her, our, civilians, quips, employee, dot-com,\n",
      "Nearest to is: was, are, remains, has, contains, cue, obscura, kido,\n",
      "Nearest to which: where, but, whose, what, cognitive, opining, that, starving,\n",
      "Nearest to UNK: la, ostrom, saurischia, pasture, ganondorf, 1500, mount, locus,\n",
      "Nearest to been: become, be, gmez, 55,000, were, debris, cranwell, pigeon,\n",
      "Nearest to the: 256, smelt, fota, buffett, cabrera, tacit, anti-semitic, a,\n",
      "Nearest to not: n't, never, conceptualization, latinos, 1868, newry, chrooting, encyclopedias,\n",
      "Nearest to are: were, is, mihly, osmond, marable, glue, four-month, correlated,\n",
      "Nearest to has: have, had, having, is, includes, e.d, hosts, suggests,\n",
      "Nearest to ,: , ;, ., 2004., refrigerate, juhu, stepson, -,\n",
      "Nearest to be: being, been, christo, raka, become, egyptians, lead, stigmasarranged,\n",
      "Nearest to one: beggars, yahweh, hadley, 2,042, mckinley, khayelitsha, shaoqi, plowed,\n",
      "Average loss at step 92000: 1.922922\n",
      "Average loss at step 94000: 1.918491\n",
      "Average loss at step 96000: 1.926949\n",
      "Average loss at step 98000: 1.910436\n",
      "Average loss at step 100000: 1.922743\n",
      "Nearest to was: is, were, became, ibac, been, being, be, 737,\n",
      "Nearest to 's: s, , empowerment, my, prolific, curtailed, , secretly,\n",
      "Nearest to ``: , discovering, patrician, duality, diracodon, storyline, taller, degrade,\n",
      "Nearest to (: lower-income, liberalised, lawless, wanda, copying, prospected, anticipate, frans,\n",
      "Nearest to their: its, his, her, our, civilians, employee, my, quips,\n",
      "Nearest to is: was, are, remains, has, allows, contains, becomes, shipwreck,\n",
      "Nearest to which: where, but, whose, opining, what, cognitive, that, actually,\n",
      "Nearest to UNK: 1500, saurischia, skip, la, locus, ostrom, 1489, mast,\n",
      "Nearest to been: become, be, gmez, 55,000, was, were, versus, debris,\n",
      "Nearest to the: anti-semitic, fenced, gamal, their, its, antisemites, buffett, columnist,\n",
      "Nearest to not: n't, never, conceptualization, newry, latinos, chrooting, 19992003, 1868,\n",
      "Nearest to are: were, is, mihly, osmond, marable, have, correlated, glue,\n",
      "Nearest to has: have, had, having, is, hosts, includes, e.d, permits,\n",
      "Nearest to ,: , ;, ., relinquish, b., mortimer, conquering, refrigerate,\n",
      "Nearest to be: being, been, christo, raka, become, argumentation, lead, was,\n",
      "Nearest to one: beggars, yahweh, hadley, 2,042, shaoqi, tempting, irinn, thrust,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "cbow_loss_unigram = []\n",
    "\n",
    "# ConfigProto is a way of providing various configuration settings \n",
    "# required to execute the graph\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    \n",
    "    # Initialize the variables in the graph\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    average_loss = 0\n",
    "    \n",
    "    # Train the Word2vec model for num_step iterations\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # Generate a single batch of data\n",
    "        batch_data, batch_labels = generate_batch_cbow(batch_size, window_size)\n",
    "        \n",
    "        # Populate the feed_dict and run the optimizer (minimize loss)\n",
    "        # and compute the loss\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        # Update the average loss variable\n",
    "        average_loss += l\n",
    "        \n",
    "        if (step+1) % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            cbow_loss_unigram.append(average_loss)\n",
    "            print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "            average_loss = 0\n",
    "            \n",
    "        # Evaluating validation set word similarities\n",
    "        if (step+1) % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            # Here we compute the top_k closest words for a given validation word\n",
    "            # in terms of the cosine distance\n",
    "            # We do this for all the words in the validation set\n",
    "            # Note: This is an expensive step\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    cbow_final_embeddings = normalized_embeddings.eval()\n",
    "    \n",
    "    \n",
    "with open('cbow_unigram_losses.csv', 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(cbow_loss_unigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsampling the Frequent words\n",
    "\n",
    "This is important as the most-frequent words such as \"in\", \"a\", \"the\" do not add a significant value to word embeddings. For example, a training input output tuple (France, Paris) has more information than (France, The). So if we can avoid such frequent words, it can help to boost the quality of word vectors. \n",
    "\n",
    "Therefore we sample each word $w_i$ with a probability $P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Word Sequence with Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 4% words (143937 words) in total...\n",
      "Dropped Examples:  ['the', ',', 'the', 'of', 'the', 'the', 'the', 'the', 'the', ',', 'the', ',', ',', 'the', 'of', 'the', ',', 'the', ',', 'the']\n",
      "\n",
      "Original data:  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing', 'the', 'opinions', 'or', 'behavior', 'of', 'large', 'numbers', 'of', 'people', '.']\n",
      "\n",
      "Subsampled data:  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing', 'the', 'opinions', 'or', 'behavior', 'of', 'large', 'numbers', 'of', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subsampled_data = []\n",
    "drop_count = 0\n",
    "drop_examples = []\n",
    "\n",
    "# Here we traverse through the data and drop irrelavent words\n",
    "# according to the subsampling probability \n",
    "for w_i in data:\n",
    "    # Note that the paper uses t=1e-5\n",
    "    # This is fine when using a normalized frequency of words\n",
    "    # But we are using raw frequencies so we set t=1e5\n",
    "    p_w_i = 1 - np.sqrt(1e5/word_count_dictionary[w_i])\n",
    "    \n",
    "    if np.random.random() < p_w_i:\n",
    "        drop_count += 1\n",
    "        drop_examples.append(reverse_dictionary[w_i])\n",
    "    else:\n",
    "        subsampled_data.append(w_i)\n",
    "        \n",
    "# Print some statistics\n",
    "print('Dropped %d%% words (%d words) in total...'%(drop_count*100.0/len(data),drop_count))\n",
    "print('Dropped Examples: ', drop_examples[:20])\n",
    "print('\\nOriginal data: ',[reverse_dictionary[w_i] for w_i in data[:20]])\n",
    "print('\\nSubsampled data: ',[reverse_dictionary[w_i] for w_i in subsampled_data[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running CBOW with Unigram Sampling + Subsampling\n",
    "\n",
    "Here we run the CBOW with unigram sampling and subsampling which we defined above. Specifically, we first initialize variables, and then train the algorithm for many steps (`num_steps`). And every few steps we evaluate the algorithm on a fixed validation set and print out the words that appear to be closest for a given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 2000: 3.433824\n",
      "Average loss at step 4000: 2.931186\n",
      "Average loss at step 6000: 2.771809\n",
      "Average loss at step 8000: 2.692310\n",
      "Average loss at step 10000: 2.646161\n",
      "Nearest to was: is, were, pronunciations, 7080, specializes, regulatory, has, conon,\n",
      "Nearest to 's: madison, chile, conquistadores, flair, feminism, |jul_rec_hi_c, jessel, elwood,\n",
      "Nearest to ``: acids, function, talons, drink, reindeer, apparatuses, encircled, greenhouse,\n",
      "Nearest to (: polygraph, thor, illuminated, gamespot, kelsen, maneuvered, hatch, maulana,\n",
      "Nearest to their: its, his, the, landmines, these, editors, 24.8, 9500,\n",
      "Nearest to is: was, are, noodles, has, municipality, detroit-windsor, eudoxus, kona,\n",
      "Nearest to which: but, kipling, mgm-produced, bubbles, championed, obstructing, theorist, who,\n",
      "Nearest to UNK: illustrating, xuanzong, 1993, hancock, improvement, feuds, tailors, bagpipes,\n",
      "Nearest to been: tech, directories, tort, terrible, hoose, argyll, hieroglyphs, prostaglandins,\n",
      "Nearest to the: its, a, their, zarathustra, an, this, terabytes, his,\n",
      "Nearest to not: also, reunited, biology, noncommercial, mid-1998, still, to, battered,\n",
      "Nearest to are: were, is, 2000., zealously, inspire, agriculturalists, overturn, kong,\n",
      "Nearest to has: have, had, is, old-fashioned, persistently, transients, antithetical, biafra,\n",
      "Nearest to ,: ;, ., skepticism, scrapped, apologies, beren, accretion, anscombe,\n",
      "Nearest to be: have, herrn, cheetham, up, kinase, reissue, were, burnside,\n",
      "Nearest to one: faulting, collection, capable, snake, gru, breakthroughs, 1766, loudly,\n",
      "Average loss at step 12000: 2.588642\n",
      "Average loss at step 14000: 2.547867\n",
      "Average loss at step 16000: 2.533857\n",
      "Average loss at step 18000: 2.494386\n",
      "Average loss at step 20000: 2.474727\n",
      "Nearest to was: is, were, has, had, specializes, pronunciations, became, baserunner,\n",
      "Nearest to 's: , flair, his, markka, madison, |jul_rec_hi_c, pasadena, oprah,\n",
      "Nearest to ``: ?, acids, crazy, function, classifications, topsoil, reindeer, cd-rom,\n",
      "Nearest to (: polygraph, =, hatch, levelled, 567, illuminated, maneuvered, gamespot,\n",
      "Nearest to their: its, his, her, these, landmines, the, 9500, any,\n",
      "Nearest to is: was, are, has, noodles, detroit-windsor, lies, unpaid, kona,\n",
      "Nearest to which: but, who, that, kipling, mgm-produced, championed, aides, bubbles,\n",
      "Nearest to UNK: folger, hancock, feuds, urge, 1993, de, dodds, tahquamenon,\n",
      "Nearest to been: tort, become, clooney, =-3, hoose, skandhas, terrible, pinpoint,\n",
      "Nearest to the: its, her, a, every, another, terabytes, their, any,\n",
      "Nearest to not: still, handed, biology, noncommercial, mid-1998, reunited, battered, n't,\n",
      "Nearest to are: were, is, have, 2000., overturn, include, zealously, agriculturalists,\n",
      "Nearest to has: have, had, is, includes, was, old-fashioned, transients, antithetical,\n",
      "Nearest to ,: ., ;, skepticism, apologies, molybdenum, scrapped, nueva, beren,\n",
      "Nearest to be: have, kinase, herrn, take, cheetham, were, reissue, sensuality,\n",
      "Nearest to one: capable, faulting, breakthroughs, collection, loudly, a, snake, seat,\n",
      "Average loss at step 22000: 2.458863\n",
      "Average loss at step 24000: 2.425538\n",
      "Average loss at step 26000: 2.412410\n",
      "Average loss at step 28000: 2.332750\n",
      "Average loss at step 30000: 2.294386\n",
      "Nearest to was: is, were, became, specializes, had, be, pronunciations, spoke,\n",
      "Nearest to 's: , his, flair, oprah, |jul_rec_hi_c, madison, markka, 2-point,\n",
      "Nearest to ``: ?, acids, classifications, crazy, encircled, spitzer, hydrogenation, reindeer,\n",
      "Nearest to (: =, polygraph, hatch, levelled, lt., illuminated, maneuvered, gamespot,\n",
      "Nearest to their: its, his, her, these, landmines, 9500, proliferating, locating,\n",
      "Nearest to is: was, are, has, lies, noodles, became, municipality, communion,\n",
      "Nearest to which: but, where, aides, kipling, who, championed, mgm-produced, leonor,\n",
      "Nearest to UNK: hancock, xuanzong, modernize, advertising, cantonal, epiglottalized, brussels-capital, folger,\n",
      "Nearest to been: become, be, =-3, clooney, tort, hoose, antagonistic, terrible,\n",
      "Nearest to the: its, every, our, their, lodging, vereeniging, aac, any,\n",
      "Nearest to not: still, n't, handed, battered, noncommercial, jokes, 294, reunited,\n",
      "Nearest to are: were, is, agriculturalists, include, zealously, inspire, have, 2000.,\n",
      "Nearest to has: have, had, is, includes, having, transients, pieterson, glomerulonephritis,\n",
      "Nearest to ,: ;, beren, mudras, 1807., apologies, ., scrapped, ecowas,\n",
      "Nearest to be: have, kinase, herrn, been, were, was, being, cheetham,\n",
      "Nearest to one: capable, faulting, another, a, loudly, semantics, battenberg, breakthroughs,\n",
      "Average loss at step 32000: 2.263320\n",
      "Average loss at step 34000: 2.264411\n",
      "Average loss at step 36000: 2.234772\n",
      "Average loss at step 38000: 2.248784\n",
      "Average loss at step 40000: 2.217367\n",
      "Nearest to was: is, were, became, specializes, had, pronunciations, spoke, has,\n",
      "Nearest to 's: , flair, his, markka, 19,000, canoes, oprah, madison,\n",
      "Nearest to ``: ?, crazy, classifications, hydrogenation, reindeer, grandest, spitzer, nabil,\n",
      "Nearest to (: polygraph, =, maneuvered, hatch, levelled, delivers, selznick, 567,\n",
      "Nearest to their: its, his, her, these, landmines, 9500, locating, decolonisation,\n",
      "Nearest to is: was, are, has, lies, detroit-windsor, became, hachani, unpaid,\n",
      "Nearest to which: but, where, mgm-produced, aides, that, kipling, championed, who,\n",
      "Nearest to UNK: xuanzong, descriptive, lansana, ripples, 21,857, improvement, advertising, mississippi,\n",
      "Nearest to been: become, be, =-3, clooney, antagonistic, tort, hoose, skandhas,\n",
      "Nearest to the: its, vereeniging, our, every, a, another, zarathustra, septic,\n",
      "Nearest to not: n't, still, handed, battered, jokes, attributions, noncommercial, 294,\n",
      "Nearest to are: were, is, agriculturalists, inspire, have, be, zealously, include,\n",
      "Nearest to has: have, had, includes, having, is, remake, was, pieterson,\n",
      "Nearest to ,: ;, ., , scrapped, mudras, beren, specializations, apologies,\n",
      "Nearest to be: kinase, being, been, have, herrn, were, reissue, cheetham,\n",
      "Nearest to one: capable, faulting, atlanta, another, wallen, a, loudly, snake,\n",
      "Average loss at step 42000: 2.209789\n",
      "Average loss at step 44000: 2.183840\n",
      "Average loss at step 46000: 2.197298\n",
      "Average loss at step 48000: 2.179137\n",
      "Average loss at step 50000: 2.162587\n",
      "Nearest to was: is, were, became, be, being, specializes, insertion, vuursche,\n",
      "Nearest to 's: , flair, his, markka, 1581, 19,000, litany, beating,\n",
      "Nearest to ``: ?, classifications, crazy, ikky, topsoil, inequalities, hydrogenation, acids,\n",
      "Nearest to (: polygraph, =, maneuvered, hatch, levelled, delivers, selznick, indices,\n",
      "Nearest to their: its, his, her, landmines, these, 9500, internal, locating,\n",
      "Nearest to is: was, are, lies, has, noodles, detroit-windsor, became, hachani,\n",
      "Nearest to which: but, where, that, aides, mgm-produced, kipling, championed, who,\n",
      "Nearest to UNK: wool, 21,857, modernize, xuanzong, hancock, tiago, fibrosis, declension,\n",
      "Nearest to been: become, be, =-3, clooney, antagonistic, was, tort, begun,\n",
      "Nearest to the: another, its, a, every, vereeniging, zarathustra, terabytes, septic,\n",
      "Nearest to not: n't, still, handed, battered, jokes, attributions, noncommercial, perceptual,\n",
      "Nearest to are: were, is, be, include, have, agriculturalists, long-period, exist,\n",
      "Nearest to has: have, had, includes, having, is, transients, remake, contains,\n",
      "Nearest to ,: ;, , ., mudras, skepticism, specializations, scrapped, indistinguishable,\n",
      "Nearest to be: being, kinase, been, herrn, was, were, are, refer,\n",
      "Nearest to one: capable, a, atlanta, loudly, wallen, another, faulting, semantics,\n",
      "Average loss at step 52000: 2.159590\n",
      "Average loss at step 54000: 2.124919\n",
      "Average loss at step 56000: 2.082569\n",
      "Average loss at step 58000: 2.081789\n",
      "Average loss at step 60000: 2.078278\n",
      "Nearest to was: is, were, became, specializes, had, spoke, present-giving, pronunciations,\n",
      "Nearest to 's: , flair, markka, his, |jul_rec_hi_c, 19,000, feminism, oprah,\n",
      "Nearest to ``: ?, crazy, classifications, inequalities, spitzer, topsoil, ikky, hydrogenation,\n",
      "Nearest to (: =, polygraph, maneuvered, hatch, bricklaying, levelled, selznick, havasu,\n",
      "Nearest to their: its, his, her, landmines, these, locating, internal, our,\n",
      "Nearest to is: was, are, lies, noodles, has, became, detroit-windsor, communion,\n",
      "Nearest to which: but, where, aides, championed, mgm-produced, that, kipling, coluim,\n",
      "Nearest to UNK: 21,857, ripples, xuanzong, brussels-capital, tiago, hancock, improvement, modernize,\n",
      "Nearest to been: become, be, =-3, antagonistic, clooney, begun, existed, terrible,\n",
      "Nearest to the: its, our, terabytes, every, their, up-to-date, septic, bled,\n",
      "Nearest to not: n't, still, attributions, handed, battered, noncommercial, jokes, never,\n",
      "Nearest to are: were, is, exist, include, have, agriculturalists, be, inspire,\n",
      "Nearest to has: have, had, having, includes, is, remake, contains, transients,\n",
      "Nearest to ,: ;, , apologies, ., mudras, indistinguishable, lsi, specializations,\n",
      "Nearest to be: being, kinase, been, herrn, have, lead, refer, go,\n",
      "Nearest to one: capable, a, atlanta, faulting, wallen, seat, loudly, another,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 62000: 2.073047\n",
      "Average loss at step 64000: 2.042571\n",
      "Average loss at step 66000: 2.051417\n",
      "Average loss at step 68000: 2.038802\n",
      "Average loss at step 70000: 2.031654\n",
      "Nearest to was: is, were, became, specializes, present-giving, spoke, had, disembodied,\n",
      "Nearest to 's: , flair, markka, kabwit, gielgud, hydrosphere, s, canoes,\n",
      "Nearest to ``: ?, classifications, inequalities, crazy, hydrogenation, pontchartrain, topsoil, palimpsest,\n",
      "Nearest to (: polygraph, =, maneuvered, hatch, delivers, levelled, havasu, selznick,\n",
      "Nearest to their: its, his, her, landmines, our, internal, 9500, locating,\n",
      "Nearest to is: was, are, lies, noodles, contains, has, detroit-windsor, became,\n",
      "Nearest to which: where, but, that, aides, championed, kipling, mgm-produced, who,\n",
      "Nearest to UNK: wool, advertising, tiago, sakaguchi, 21,857, hancock, improvement, tahquamenon,\n",
      "Nearest to been: become, be, =-3, begun, clooney, antagonistic, existed, evolved,\n",
      "Nearest to the: our, vereeniging, every, hip-hop, bled, terabytes, septic, zarathustra,\n",
      "Nearest to not: n't, still, never, perceptual, attributions, handed, jokes, noncommercial,\n",
      "Nearest to are: were, is, exist, have, include, agriculturalists, remain, decatur,\n",
      "Nearest to has: have, had, having, includes, contains, is, remake, hamlet-like,\n",
      "Nearest to ,: ;, ., , scrapped, specializations, indistinguishable, beren, skepticism,\n",
      "Nearest to be: being, kinase, been, herrn, refer, lead, go, drop,\n",
      "Nearest to one: capable, atlanta, wallen, seat, akihito, gru, loudly, sciabarra,\n",
      "Average loss at step 72000: 2.026486\n",
      "Average loss at step 74000: 2.017159\n",
      "Average loss at step 76000: 2.039677\n",
      "Average loss at step 78000: 1.999701\n",
      "Average loss at step 80000: 1.987775\n",
      "Nearest to was: is, were, spoke, became, specializes, present-giving, came, vuursche,\n",
      "Nearest to 's: , flair, his, kabwit, s, oprah, markka, madison,\n",
      "Nearest to ``: ?, classifications, depicting, ikky, topsoil, crazy, palimpsest, grandest,\n",
      "Nearest to (: polygraph, =, maneuvered, hatch, levelled, delivers, civitas, kitchen,\n",
      "Nearest to their: its, his, her, our, landmines, internal, locating, decolonisation,\n",
      "Nearest to is: was, are, lies, noodles, has, detroit-windsor, contains, remains,\n",
      "Nearest to which: where, but, aides, that, championed, kipling, mgm-produced, d'alviano,\n",
      "Nearest to UNK: hancock, wool, 21,857, tiago, fungicides, e, panyu, lansana,\n",
      "Nearest to been: become, be, =-3, existed, antagonistic, evolved, begun, clooney,\n",
      "Nearest to the: septic, zarathustra, vereeniging, every, tian, tithing, futuna, hip-hop,\n",
      "Nearest to not: n't, still, never, attributions, perceptual, jokes, handed, battered,\n",
      "Nearest to are: were, is, exist, remain, agriculturalists, include, zealously, inspire,\n",
      "Nearest to has: have, had, includes, having, contains, is, remake, glomerulonephritis,\n",
      "Nearest to ,: , ;, apologies, mudras, scrapped, beren, musique, photographers,\n",
      "Nearest to be: being, been, kinase, herrn, refer, lead, reissue, go,\n",
      "Nearest to one: capable, atlanta, seat, akihito, faulting, another, wallen, sciabarra,\n",
      "Average loss at step 82000: 1.940703\n",
      "Average loss at step 84000: 1.961399\n",
      "Average loss at step 86000: 1.953416\n",
      "Average loss at step 88000: 1.950603\n",
      "Average loss at step 90000: 1.907099\n",
      "Nearest to was: is, were, specializes, became, spoke, present-giving, had, pronunciations,\n",
      "Nearest to 's: , flair, s, markka, kabwit, litany, 19,000, hydrosphere,\n",
      "Nearest to ``: ?, classifications, depicting, ikky, topsoil, palimpsest, crazy, pontchartrain,\n",
      "Nearest to (: polygraph, =, maneuvered, hatch, delivers, levelled, selznick, proteges,\n",
      "Nearest to their: its, his, her, our, landmines, internal, locating, decolonisation,\n",
      "Nearest to is: was, are, lies, noodles, has, remains, contains, consists,\n",
      "Nearest to which: where, but, aides, championed, mgm-produced, that, whom, d'alviano,\n",
      "Nearest to UNK: hancock, tiago, 21,857, lansana, e, ripples, von, wool,\n",
      "Nearest to been: become, be, =-3, existed, evolved, begun, antagonistic, clooney,\n",
      "Nearest to the: terabytes, vereeniging, yat-sen., its, your, riyadh, bletchley, andrei,\n",
      "Nearest to not: n't, never, still, jokes, attributions, battered, perceptual, handed,\n",
      "Nearest to are: were, is, remain, exist, no-fly, agriculturalists, resembles, inspire,\n",
      "Nearest to has: have, had, having, includes, contains, is, remake, glomerulonephritis,\n",
      "Nearest to ,: , ;, mudras, ., specializations, beren, nueva, satisfactorily,\n",
      "Nearest to be: being, been, kinase, refer, herrn, lead, reissue, join,\n",
      "Nearest to one: capable, atlanta, seat, akihito, wallen, faulting, sciabarra, gru,\n",
      "Average loss at step 92000: 1.938452\n",
      "Average loss at step 94000: 1.913899\n",
      "Average loss at step 96000: 1.919135\n",
      "Average loss at step 98000: 1.908216\n",
      "Average loss at step 100000: 1.914178\n",
      "Nearest to was: is, were, became, spoke, specializes, vuursche, being, present-giving,\n",
      "Nearest to 's: , flair, s, markka, kabwit, romanised, neale, 19,000,\n",
      "Nearest to ``: ?, classifications, depicting, topsoil, ikky, hydrogenation, crazy, scaled,\n",
      "Nearest to (: polygraph, maneuvered, delivers, levelled, =, hatch, civitas, havasu,\n",
      "Nearest to their: its, his, her, landmines, our, internal, locating, 9500,\n",
      "Nearest to is: was, are, lies, remains, noodles, contains, detroit-windsor, has,\n",
      "Nearest to which: where, but, aides, that, whom, championed, whose, psychoanalytical,\n",
      "Nearest to UNK: hancock, wool, 21,857, tahquamenon, hanbali, e, urge, xuanzong,\n",
      "Nearest to been: become, be, existed, =-3, evolved, antagonistic, clooney, begun,\n",
      "Nearest to the: vereeniging, terabytes, your, septic, zarathustra, every, a, yat-sen.,\n",
      "Nearest to not: n't, never, still, perceptual, jokes, handed, battered, attributions,\n",
      "Nearest to are: were, is, remain, exist, agriculturalists, no-fly, decatur, inspire,\n",
      "Nearest to has: have, had, having, includes, contains, is, remake, maintains,\n",
      "Nearest to ,: , ;, specializations, mudras, ., scrapped, nueva, beren,\n",
      "Nearest to be: being, been, kinase, refer, lead, remain, drop, herrn,\n",
      "Nearest to one: capable, sciabarra, atlanta, akihito, seat, loudly, wallen, gru,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "cbow_loss_unigram_subsampled = []\n",
    "\n",
    "\n",
    "# ConfigProto is a way of providing various configuration settings \n",
    "# required to execute the graph\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    \n",
    "    # Initialize the variables in the graph\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    average_loss = 0\n",
    "    \n",
    "    # Train the Word2vec model for num_step iterations\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # Generate a single batch of data\n",
    "        batch_data, batch_labels = generate_batch_cbow(batch_size, window_size)\n",
    "        \n",
    "        # Populate the feed_dict and run the optimizer (minimize loss)\n",
    "        # and compute the loss\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        # Update the average loss variable\n",
    "        average_loss += l\n",
    "        \n",
    "        if (step+1) % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            cbow_loss_unigram_subsampled.append(average_loss)\n",
    "            print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "            average_loss = 0\n",
    "            \n",
    "        # Evaluating validation set word similarities\n",
    "        if (step+1) % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            # Here we compute the top_k closest words for a given validation word\n",
    "            # in terms of the cosine distance\n",
    "            # We do this for all the words in the validation set\n",
    "            # Note: This is an expensive step\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    cbow_final_embeddings = normalized_embeddings.eval()\n",
    "    \n",
    "    \n",
    "with open('cbow_unigram_subsampled_losses.csv', 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(cbow_loss_unigram_subsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plotting CBOW (Original), Unigram based Negative Sampling and Subsampling Losses\n",
    "\n",
    "Here we plot the losses obtained over time for all the CBOW based algorithms to see which one performs better. We provide a detailed analysis of the observations in the Chapter 4 text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.figure(figsize=(15,5))  # in inches\n",
    "\n",
    "# Define the x axis\n",
    "x = np.arange(len(skip_gram_loss))*2000\n",
    "\n",
    "# Plotting standard CBOW loss, CBOW loss with unigram sampling and\n",
    "# CBOW loss with unigram sampling + subsampling here in one plot\n",
    "pylab.plot(x, cbow_loss, label=\"CBOW\",linestyle='--',linewidth=2)\n",
    "pylab.plot(x, cbow_loss_unigram, label=\"CBOW (Unigram)\",linestyle='-.',linewidth=2,marker='^',markersize=5)\n",
    "pylab.plot(x, cbow_loss_unigram_subsampled, label=\"CBOW (Unigram+Subsampling)\",linewidth=2)\n",
    "\n",
    "# Some text around the plots\n",
    "pylab.title('Original CBOW vs Various Improvements Loss Decrease Over-Time',fontsize=24)\n",
    "pylab.xlabel('Iterations',fontsize=22)\n",
    "pylab.ylabel('Loss',fontsize=22)\n",
    "pylab.legend(loc=1,fontsize=22)\n",
    "\n",
    "# Use for saving the figure if needed\n",
    "pylab.savefig('loss_cbow_vs_all_improvements.png')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow1.15-cpu]",
   "language": "python",
   "name": "conda-env-tensorflow1.15-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
